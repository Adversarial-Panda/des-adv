{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6257906-ce05-462f-aaa9-e5373283d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets \n",
    "from torch.utils.data import DataLoader\n",
    "# from medmnist import INFO\n",
    "import numpy as np\n",
    "import faiss\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.functional import softmax, cosine_similarity\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a604dd-b061-4ea9-b31f-80263d262fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fef7d91-1d3a-4c8a-9795-18449dbeab74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a96c5ca-7163-4655-85d2-02bee465bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Allow Subset class for unpickling\n",
    "torch.serialization.add_safe_globals([Subset])\n",
    "\n",
    "# Load the datasets\n",
    "test_subset = torch.load(\"data/cifar10_selected_test.pt\", weights_only=False)\n",
    "val_subset  = torch.load(\"data/cifar10_extended_val.pt\", weights_only=False)\n",
    "\n",
    "testloader = DataLoader(test_subset, batch_size=1, shuffle=False)\n",
    "valloader  = DataLoader(val_subset,  batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfbcc-176f-4eee-9aa7-29ce36e98e96",
   "metadata": {},
   "source": [
    "### Load Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa28db3d-14df-4567-8155-7667dacef2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Loading vit_tiny_patch16_224 from Models/vit_tiny.pth.tar\n",
      "Load result: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import model_helper as helper\n",
    "\n",
    "ens_models_args = [\n",
    "    # \"resnet18\", \n",
    "    \"inception_v3\", \n",
    "    # \"deit_tiny_patch16_224\", \n",
    "    \"vit_tiny_patch16_224\", \n",
    "    \"efficientnet_b0\", \n",
    "    \"gcvit_tiny\"\n",
    "    ]\n",
    "\n",
    "ens_models = [] \n",
    "\n",
    "for i in ens_models_args: \n",
    "    model = helper.load_model_hub(i)\n",
    "    model = model.to(device)\n",
    "    ens_models.append(model.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8699e-65e7-4f60-a334-0f2b15965023",
   "metadata": {},
   "source": [
    "### Ensemble Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06c3b0d8-2e7e-4e8c-8e90-fd119104fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='SMER')\n",
    "\n",
    "    parser.add_argument('--dataset', type=str, default='imagenet_compatible')\n",
    "    parser.add_argument('--batch-size', type=int, default=50)\n",
    "    parser.add_argument('--image-size', type=int, default=224)\n",
    "    parser.add_argument('--num_worker', type=int, default=4)\n",
    "    parser.add_argument('--attack_method', type=str, default='MI_FGSM_SMER')\n",
    "    parser.add_argument('--image-dir', type=str)\n",
    "    parser.add_argument('--image-info', type=str, default='')\n",
    "    parser.add_argument('--gpu-id', type=int, default=0)\n",
    "\n",
    "    # attack params\n",
    "    parser.add_argument('--eps', type=float, default=8.0)\n",
    "    parser.add_argument('--alpha', type=float, default=2)\n",
    "    parser.add_argument('--iters', type=int, default=10)\n",
    "    parser.add_argument('--momentum', type=float, default=1.0)\n",
    "    parser.add_argument('--beta', type=float, default=10)\n",
    "\n",
    "    # FIX for Jupyter\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "# Correct call\n",
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52d730ec-5a72-46f4-84e0-366e094de86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating MI-FGSM adversarials (GPU): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [2:28:46<00:00,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 1000 adversarial images. Shape: torch.Size([1000, 3, 32, 32])\n",
      "Noise (1 - SSIM): mean=0.097560, std=0.071412, min=0.000000, max=0.579211\n",
      "Mean absolute pixel diff (after clamp to [0,1]): mean=0.025420, std=0.009533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ensemble_attacks\n",
    "from torchmetrics.functional.image import structural_similarity_index_measure as ssim\n",
    "import torch\n",
    "\n",
    "# --- before loop (clear previous lists) ---\n",
    "adv_list = []\n",
    "orig_list = []\n",
    "labels_list = []\n",
    "noise_rates = []\n",
    "pixel_diffs = []\n",
    "\n",
    "max_value = 1.0 \n",
    "min_value = 0.0 \n",
    "eps = 8/255\n",
    "alpha = 2/255 \n",
    "iters = 10 \n",
    "threshold = -0.3\n",
    "beta = 10 \n",
    "\n",
    "attack_method = ensemble_attacks.AdaEA_MIFGSM(ens_models, eps=eps, alpha=alpha, iters=iters, max_value=max_value, \n",
    "                             min_value=min_value, beta=beta, threshold=threshold, device=device)\n",
    "\n",
    "def ensure_batch(x):\n",
    "    return x if x.dim() == 4 else x.unsqueeze(0)\n",
    "\n",
    "def to_unit_range(x):\n",
    "    \"\"\"\n",
    "    Ensure x is in [0,1]. If tensor values appear to be in [0,255] (max>1.5),\n",
    "    convert by dividing by 255. Returns a float tensor on same device.\n",
    "    \"\"\"\n",
    "    x = ensure_batch(x).float()\n",
    "    if x.max().item() > 1.5:\n",
    "        x = x / 255.0\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "# --- attack loop (same as yours, but using the simplified functions) ---\n",
    "for img, label in tqdm(testloader, desc=\"Generating MI-FGSM adversarials (GPU)\"):\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # adv_img = ensemble_attacks.ensemble_mi_fgsm(ens_models, img, label, eps=eps, alpha=alpha, iters=iters, clip_min=0.0, clip_max=1.0, device=device)\n",
    "        # adv_img = ensemble_attacks.ensemble_svre_mi_fgsm(\n",
    "        #     ens_models, img, label,\n",
    "        #     eps=8/255, alpha=2/255, iters=10,\n",
    "        #     decay=1.0, sample_k=2, refresh=5\n",
    "        # )\n",
    "        # adv_img = attack_method(img, label) \n",
    "        adv_img = ensemble_attacks.MI_FGSM_SMER(\n",
    "            ens_models,\n",
    "            img,\n",
    "            label,\n",
    "            args,\n",
    "            num_iter=10\n",
    "        )\n",
    "        \n",
    "    # store for later (move to CPU)\n",
    "    adv_list.append(adv_img.squeeze(0).cpu())\n",
    "    orig_list.append(img.squeeze(0).cpu())\n",
    "    labels_list.append(label.squeeze(0).cpu())\n",
    "\n",
    "    # compute SSIM and pixel diffs on [0,1] images\n",
    "    img_for_ssim = to_unit_range(img)       # (1,C,H,W) in [0,1]\n",
    "    adv_for_ssim = to_unit_range(adv_img)   # (1,C,H,W) in [0,1]\n",
    "\n",
    "    ssim_val = ssim(adv_for_ssim, img_for_ssim)  # scalar tensor\n",
    "    noise_rates.append((1.0 - float(ssim_val)))\n",
    "    pixel_diffs.append((adv_for_ssim - img_for_ssim).abs().mean().item())\n",
    "\n",
    "# --- stack everything on CPU ---\n",
    "adv_all = torch.stack(adv_list).cpu()\n",
    "orig_all = torch.stack(orig_list).cpu()\n",
    "labels_all = torch.stack(labels_list).cpu()\n",
    "\n",
    "noise_rates = torch.tensor(noise_rates)\n",
    "pixel_diffs = torch.tensor(pixel_diffs)\n",
    "\n",
    "print(f\"âœ… Generated {adv_all.size(0)} adversarial images. Shape: {adv_all.shape}\")\n",
    "print(f\"Noise (1 - SSIM): mean={noise_rates.mean():.6f}, std={noise_rates.std():.6f}, min={noise_rates.min():.6f}, max={noise_rates.max():.6f}\")\n",
    "print(f\"Mean absolute pixel diff (after clamp to [0,1]): mean={pixel_diffs.mean():.6f}, std={pixel_diffs.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209c592-ed2c-49d9-96a0-75e9bcb3e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "âœ… Generated 1000 adversarial images. Shape: torch.Size([1000, 3, 32, 32])\n",
    "Noise (1 - SSIM): mean=0.085751, std=0.055479, min=0.000056, max=0.512114\n",
    "Mean absolute pixel diff (after clamp to [0,1]): mean=0.023210, std=0.002622 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c590fc-c3f8-43f5-8af3-35dc20a85c81",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "954ae92f-01b8-4085-bb98-9749b591e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32 \n",
    "dataset = TensorDataset(adv_all, labels_all)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1b8b0a1-1f4e-4a6d-8877-3a9245f36ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Loading vit_base_patch16_224 from Models/target/vit_base.pth.tar\n",
      "Load result: <All keys matched successfully>\n",
      "\n",
      "ðŸ”¹ Loading deit_base_patch16_224 from Models/target/deit_base.pth.tar\n",
      "Load result: <All keys matched successfully>\n",
      "\n",
      "ðŸ”¹ Loading swin_base_patch4_window7_224 from Models/target/swin_base.pth.tar\n",
      "Load result: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import model_helper as helper\n",
    "\n",
    "target_models_args = [\n",
    "    # \"resnetv2_101x1_bitm\", \n",
    "    # \"resnet152\", \n",
    "    # \"regnety_160\", \n",
    "    \"vit_base_patch16_224\", \n",
    "    \"deit_base_patch16_224\", \n",
    "    \"swin_base_patch4_window7_224\", \n",
    "    \"convmixer_768_32\"\n",
    "    ]\n",
    "\n",
    "target_models = [] \n",
    "\n",
    "for i in target_models_args: \n",
    "    model = helper.load_model_hub(i)\n",
    "    model = model.to(device)\n",
    "    target_models.append(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c0db262-0d94-452f-9a37-7ec15fab8cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 65.10%  (651/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:03<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 70.20%  (702/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 76.90%  (769/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:07<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 81.60%  (816/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for t_model in target_models:\n",
    "        name = getattr(t_model, \"name\", t_model.__class__.__name__)\n",
    "        t_model.eval()\n",
    "        t_model.to(device)\n",
    "\n",
    "        fooled = 0\n",
    "        total = 0\n",
    "\n",
    "        for imgs_cpu, labels_cpu in tqdm(loader, desc=f\"ASR {name}\"):\n",
    "            # Move to device here\n",
    "            imgs = imgs_cpu.to(device, non_blocking=True)\n",
    "            labels = labels_cpu.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = t_model(imgs)\n",
    "            if isinstance(outputs, (tuple, list)):\n",
    "                outputs = outputs[0]\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            fooled += (preds != labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # free cache per batch (helps on tight GPUs)\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        asr = 100.0 * fooled / total if total > 0 else 0.0\n",
    "        print(f\"{name}: ASR = {asr:.2f}%  ({fooled}/{total} fooled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54c443-4eec-4925-8027-098f547d3ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708e3a6-b660-4828-8116-b6fdd357d8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
