{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5376af6-b146-4fee-8484-e8dcce2bc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58498bad-6432-4afc-82d3-e98604d4a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_models(dataset, model_name, key):\n",
    "    if dataset == 'cifar10':\n",
    "        # For Inception, resize to 299x299; otherwise 32x32 is fine\n",
    "        if any(x in key for x in ['inc', 'bit']):\n",
    "            transform_resize = transforms.Resize((299, 299))\n",
    "            norm = transforms.Normalize((0.5,), (0.5,)) \n",
    "        elif any(x in key for x in ['vit']):\n",
    "            transform_resize = transforms.Resize((224, 224))\n",
    "            norm = transforms.Normalize((0.5,), (0.5,)) \n",
    "        else:\n",
    "            transform_resize = transforms.Resize((224, 224)) # do nothing\n",
    "            norm = transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                        (0.2023, 0.1994, 0.2010)) \n",
    "    \n",
    "        # Create model\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=10).to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Wrap resize + normalization + model in Sequential\n",
    "        return torch.nn.Sequential(\n",
    "            transform_resize,   # resize if Inception\n",
    "            norm,               # normalization\n",
    "            model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95261260-e43d-46f4-b2d7-18392a50ad9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haloregnetz_b',\n",
       " 'nf_regnet_b0',\n",
       " 'nf_regnet_b1',\n",
       " 'nf_regnet_b2',\n",
       " 'nf_regnet_b3',\n",
       " 'nf_regnet_b4',\n",
       " 'nf_regnet_b5',\n",
       " 'regnetv_040',\n",
       " 'regnetv_064',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_040s_gn',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'regnetz_005',\n",
       " 'regnetz_040',\n",
       " 'regnetz_040h',\n",
       " 'regnetz_b16',\n",
       " 'regnetz_b16_evos',\n",
       " 'regnetz_c16',\n",
       " 'regnetz_c16_evos',\n",
       " 'regnetz_d8',\n",
       " 'regnetz_d8_evos',\n",
       " 'regnetz_d32',\n",
       " 'regnetz_e8']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*regnet*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8815c74-67ea-407a-895a-527aa302236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 45000, Val: 5000, Test: 10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# CIFAR-10 normalization for training and test\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "    #                      (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "    #                      (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Split train into train/val\n",
    "train_size = int(0.9 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size], generator=torch.Generator().manual_seed(56))\n",
    "\n",
    "# DataLoaders\n",
    "trainloader = DataLoader(train_subset, batch_size=48, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_subset, batch_size=48, shuffle=False, num_workers=4)\n",
    "testloader = DataLoader(testset, batch_size=48, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d427e-13bc-4f11-a031-aa65c51d92d7",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc8fd0f-06c6-4ba6-b9b5-31d5454610f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    trainloader,\n",
    "    valloader,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    checkpoint_path=\"best_model.pth\",\n",
    "    early_stop_patience=None,\n",
    "):\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler()     # Mixed precision\n",
    "\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, total, correct = 0, 0, 0\n",
    "\n",
    "        pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():   # FP16 forward\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        val_acc = evaluate(model, valloader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Loss={running_loss/total:.4f}, \"\n",
    "            f\"Train Acc={train_acc:.2f}%, \"\n",
    "            f\"Val Acc={val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------\n",
    "        # Save Best Model Checkpoint\n",
    "        # -------------------------------\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"val_acc\": best_val_acc,\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            )\n",
    "            print(f\"✨ Saved Best Model (Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # -------------------------------\n",
    "        # Early Stopping\n",
    "        # -------------------------------\n",
    "        if early_stop_patience is not None:\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(\"⛔ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Training completed. Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c982a0b-2b04-4921-8c44-ccd875c13861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y.size(0)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d4698-63ea-4b2c-a99a-468835b4d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-morevit/gcvit_tiny_224_nvidia-ac783954.pth\" to /home/firuz/.cache/torch/hub/checkpoints/gcvit_tiny_224_nvidia-ac783954.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|███████████████████████████████| 938/938 [03:00<00:00,  5.21it/s, loss=0.033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.3747, Train Acc=87.50%, Val Acc=90.18%\n",
      "✨ Saved Best Model (Val Acc: 90.18%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|███████████████████████████████| 938/938 [02:53<00:00,  5.40it/s, loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.2300, Train Acc=92.39%, Val Acc=92.60%\n",
      "✨ Saved Best Model (Val Acc: 92.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|███████████████████████████████| 938/938 [02:56<00:00,  5.30it/s, loss=0.182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.1931, Train Acc=93.63%, Val Acc=93.06%\n",
      "✨ Saved Best Model (Val Acc: 93.06%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|███████████████████████████████| 938/938 [02:53<00:00,  5.40it/s, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.1675, Train Acc=94.45%, Val Acc=91.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|███████████████████████████████| 938/938 [03:15<00:00,  4.79it/s, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.1548, Train Acc=94.86%, Val Acc=91.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|███████████████████████████████| 938/938 [02:53<00:00,  5.39it/s, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.1463, Train Acc=95.13%, Val Acc=93.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|███████████████████████████████| 938/938 [02:54<00:00,  5.39it/s, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.1190, Train Acc=96.02%, Val Acc=92.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50:  51%|████████████████▏               | 476/938 [01:28<01:25,  5.41it/s, loss=0.25]"
     ]
    }
   ],
   "source": [
    "model_name = \"gcvit_tiny\" \n",
    "\n",
    "model = get_models('cifar10', model_name, \"vit\") \n",
    "\n",
    "train_model(model, trainloader, valloader, epochs=50, lr=1e-3, checkpoint_path=f\"checkpoints/{model_name}_cifar10.pth\", \n",
    "           early_stop_patience=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e826dc2-4c20-4f34-a460-053c898b2d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
