{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee80a2e8-a2db-4ead-851f-a97d81b5d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# from medmnist import INFO\n",
    "import numpy as np\n",
    "import faiss\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.functional import softmax, cosine_similarity\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35b066e-e2c8-45ef-bd51-53bb41219b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaff40-c513-4fa9-a0f2-dc19f0cd79f1",
   "metadata": {},
   "source": [
    "### Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674253ec-a779-45af-bd6e-c5cdce32cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading dataset with resize transform...\n",
      "Validation samples: 30000\n",
      "Validation samples: 2004\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "print(f\"Step 1: Loading dataset with resize transform...\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize([0.485, 0.456, 0.406],\n",
    "    #                      [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "val_data_dir = 'dataset/imagenet_validation' \n",
    "test_data_dir = 'dataset/imagenet_tests'\n",
    "\n",
    "val_dataset = datasets.ImageFolder(os.path.join(val_data_dir), transform=transform) \n",
    "\n",
    "test_datasets = {\n",
    "    f'test{i}': datasets.ImageFolder(os.path.join(test_data_dir, f'test{i}'), transform=transform)\n",
    "    for i in range(1, 11)\n",
    "}\n",
    "\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Validation samples: {len(test_datasets['test1'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e78a99a-5757-4f2c-953a-5d48ffded78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4) \n",
    "test_loader = DataLoader(test_datasets['test1'], batch_size=1, shuffle=False, num_workers=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf8141-2bb2-461a-b452-4339756429e7",
   "metadata": {},
   "source": [
    "### Ensemble Attack (ICLR-17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f018f09-22b6-43ab-9a56-17fd64471021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "def ensemble_ifgsm(\n",
    "    models: List[torch.nn.Module],\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    eps: float = 8/255,\n",
    "    alpha: float = 2/255,\n",
    "    iters: int = 10,\n",
    "    clip_min: float = 0.0,\n",
    "    clip_max: float = 1.0,\n",
    "    loss_fn = None,\n",
    "    device: str = None,\n",
    "):\n",
    "\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "\n",
    "    # ensure models is a list\n",
    "    if not isinstance(models, list) and not isinstance(models, tuple):\n",
    "        models = [models]\n",
    "\n",
    "    # move models to device and set eval\n",
    "    for m in models:\n",
    "        m.to(device)\n",
    "        m.eval()\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    x_orig = x.clone().detach().to(device)\n",
    "    x_adv = x_orig.clone().detach()\n",
    "\n",
    "    # For stability, operate in float32\n",
    "    x_adv = x_adv.float()\n",
    "    x_orig = x_orig.float()\n",
    "    y = y.to(device)\n",
    "\n",
    "    # iterative attack\n",
    "    for i in range(iters):\n",
    "        # enable grad on the adversarial tensor\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "        # compute ensemble loss: average of per-model losses\n",
    "        ensemble_loss = 0.0\n",
    "        for m in models:\n",
    "            out = m(x_adv)\n",
    "            # handle tuple outputs (e.g. inception)\n",
    "            if isinstance(out, (tuple, list)):\n",
    "                out = out[0]\n",
    "            ensemble_loss = ensemble_loss + loss_fn(out, y)\n",
    "        ensemble_loss = ensemble_loss / len(models)\n",
    "\n",
    "        # compute gradients\n",
    "        grad = torch.autograd.grad(ensemble_loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        # step: L_inf sign update (untargeted: ascend loss)\n",
    "        step = alpha * grad.sign()\n",
    "        x_adv = x_adv.detach() + step.detach()\n",
    "\n",
    "        # project to L_inf ball around original\n",
    "        delta = torch.clamp(x_adv - x_orig, min=-eps, max=eps)\n",
    "        x_adv = torch.clamp(x_orig + delta, min=clip_min, max=clip_max).detach()\n",
    "\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ccf12-17f2-40bb-b464-77798470bb57",
   "metadata": {},
   "source": [
    "### Prepare for Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9906779a-72e5-4148-a5d5-bde6ce3d516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(dataset, model_name, key): \n",
    "    if dataset == 'imagenet':\n",
    "        # save_root_path = r\"checkpoint/tinyimagenet\"\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=1000).to(device)\n",
    "        model.eval()\n",
    "        if 'inc' in key or 'vit' in key or 'bit' in key:\n",
    "            return torch.nn.Sequential(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), model)\n",
    "        else:\n",
    "            return torch.nn.Sequential(transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1308b805-c497-4166-bd67-e4d20a48a557",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_models = [\n",
    "    get_models(\"imagenet\", \"resnet18\", \"resnet18\"), \n",
    "    get_models(\"imagenet\", \"deit_tiny_patch16_224\", \"deit_t\"),\n",
    "    get_models(\"imagenet\", \"inception_v3\", \"inc_v3\"), \n",
    "    get_models(\"imagenet\", \"vit_tiny_patch16_224\", \"vit_t\"), \n",
    "    # get_models(\"imagenet\", \"efficientnet_b0\", \"efficientnet_b0\"), \n",
    "    # get_models(\"imagenet\", \"swin_tiny_patch4_window7_224\", \"swin_t\"), \n",
    "    # get_models(\"imagenet\", \"xcit_tiny_12_p8_224\", \"swin_t\"), \n",
    "    # get_models(\"imagenet\", \"regnetv_040\", \"swin_t\"), \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0981d970-468f-493e-b51f-fda2ed547df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [34:28<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "adv_images = []     \n",
    "adv_labels = []\n",
    "\n",
    "eps = 8/255\n",
    "alpha = 2/255\n",
    "iters = 10\n",
    "\n",
    "for data, label in tqdm(test_loader):\n",
    "    data, label = data.to(device), label.to(device)\n",
    "    # generate adv on batch (works for batch_size > 1)\n",
    "    adv = ensemble_ifgsm(ens_models, data, label, eps=eps, alpha=alpha, iters=iters, clip_min=0.0, clip_max=1.0)\n",
    "    # detach and move to CPU to avoid OOM\n",
    "    adv_images.append(adv.detach().cpu())\n",
    "    adv_labels.append(label.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e789a-c409-4002-9dd3-f5ab712e8d87",
   "metadata": {},
   "source": [
    "### Test on DES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb48df1-7da5-4dcf-b05d-3382eed49f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedModel(nn.Module):\n",
    "    def __init__(self, model, mean, std):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.register_buffer('mean', torch.tensor(mean).view(1,3,1,1))\n",
    "        self.register_buffer('std', torch.tensor(std).view(1,3,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.mean) / self.std\n",
    "        return self.model(x)\n",
    "\n",
    "def get_last_linear_layer(model):\n",
    "    \"\"\"\n",
    "    Find the last Linear layer in timm models or wrapped models (NormalizedModel).\n",
    "    \"\"\"\n",
    "    # unwrap NormalizedModel if needed\n",
    "    if isinstance(model, NormalizedModel):\n",
    "        model = model.model\n",
    "\n",
    "    # Common attribute names for classifier heads\n",
    "    candidate_attrs = ['head', 'heads', 'fc', 'classifier', 'mlp_head']\n",
    "\n",
    "    for attr in candidate_attrs:\n",
    "        if hasattr(model, attr):\n",
    "            layer = getattr(model, attr)\n",
    "            # If it's a Linear layer\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                return layer\n",
    "            # If it's a Sequential or Module, search inside\n",
    "            if isinstance(layer, nn.Module):\n",
    "                last_linear = None\n",
    "                for m in reversed(list(layer.modules())):\n",
    "                    if isinstance(m, nn.Linear):\n",
    "                        last_linear = m\n",
    "                        break\n",
    "                if last_linear is not None:\n",
    "                    return last_linear\n",
    "\n",
    "    # Fallback: scan all modules\n",
    "    last_linear = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            last_linear = m\n",
    "    if last_linear is not None:\n",
    "        return last_linear\n",
    "\n",
    "    raise RuntimeError(f\"No Linear layer found in model {model.__class__.__name__}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_features_before_last_linear(model, x):\n",
    "    \"\"\"\n",
    "    Extract features before the final classifier, works for CNNs and ViTs.\n",
    "    \"\"\"\n",
    "    # unwrap NormalizedModel if present\n",
    "    if isinstance(model, NormalizedModel):\n",
    "        model = model.model\n",
    "\n",
    "    # Common classifier attributes\n",
    "    candidate_attrs = ['head', 'heads', 'fc', 'classifier', 'mlp_head']\n",
    "    classifier = None\n",
    "    for attr in candidate_attrs:\n",
    "        if hasattr(model, attr):\n",
    "            classifier = getattr(model, attr)\n",
    "            break\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        features['feat'] = input[0].detach()\n",
    "\n",
    "    if classifier is not None:\n",
    "        handle = classifier.register_forward_hook(hook)\n",
    "    else:\n",
    "        # fallback: attach hook to last module\n",
    "        last_module = list(model.modules())[-1]\n",
    "        handle = last_module.register_forward_hook(hook)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    handle.remove()\n",
    "\n",
    "    if 'feat' not in features:\n",
    "        raise RuntimeError(f\"Failed to capture features from model {model.__class__.__name__}\")\n",
    "\n",
    "    return features['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccbcc6b8-bd5e-41ed-9b28-4db2e2dbf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_check(local_labels, preds, per_class_min=1):\n",
    "    local_labels = np.asarray(local_labels)\n",
    "\n",
    "    # If preds are logits/probs, convert to labels\n",
    "    preds = np.asarray(preds)\n",
    "    if preds.ndim > 1:\n",
    "        preds = preds.argmax(axis=1)\n",
    "\n",
    "    # Classes present in the RoC (unique, not repeated)\n",
    "    classes_in_roc = np.unique(local_labels)\n",
    "\n",
    "    # Check: for each class c in RoC, there is at least `per_class_min` correct prediction\n",
    "    missing = []\n",
    "    for c in classes_in_roc:\n",
    "        mask = (local_labels == c)\n",
    "        n_correct = int(np.sum(preds[mask] == c))\n",
    "        if n_correct < per_class_min:\n",
    "            missing.append((int(c), n_correct))  # track which class is short\n",
    "\n",
    "    fire_ok = (len(missing) == 0)\n",
    "    return fire_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ba6e0f-9b2b-4a80-803a-a6d10017c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softmax(x, dim=0):\n",
    "    return F.softmax(x, dim=dim)\n",
    "\n",
    "\n",
    "def cosine_similarity(x, y, dim=1, eps=1e-8):\n",
    "    return F.cosine_similarity(x, y, dim=dim, eps=eps)\n",
    "\n",
    "\n",
    "# ðŸ‘€ Dummy visualization (replace with your function)\n",
    "def visualize_test_and_roc(test_img, roc_imgs, local_labels):\n",
    "    print(\"Visualization placeholder: Test image + RoC samples\")\n",
    "    print(f\"RoC labels: {local_labels}\")\n",
    "\n",
    "\n",
    "class VisionDES_2: \n",
    "    def __init__(self, dsel_dataset, pool): \n",
    "        self.dsel_dataset = dsel_dataset\n",
    "        self.dsel_loader = DataLoader(dsel_dataset, batch_size=32, shuffle=False) \n",
    "        self.dino_model = timm.create_model('vit_base_patch16_224.dino', pretrained=True).to(device)\n",
    "        self.dino_model.eval()  \n",
    "        self.pool = pool \n",
    "\n",
    "        self.suspected_model_votes = [] \n",
    "        \n",
    "        \n",
    "    def dino_embedder(self, images):\n",
    "        if images.shape[1] == 1:\n",
    "            images = images.repeat(1, 3, 1, 1)\n",
    "        return self.dino_model.forward_features(images)\n",
    "\n",
    "\n",
    "    def fit(self): \n",
    "        dsel_embeddings = []\n",
    "        dsel_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.dsel_loader):\n",
    "                imgs = imgs.to(device)\n",
    "                embs = self.dino_embedder(imgs).cpu()  \n",
    "                dsel_embeddings.append(embs)\n",
    "                dsel_labels.append(labels)\n",
    "    \n",
    "        # Keep as tensor\n",
    "        dsel_embeddings_tensor = torch.cat(dsel_embeddings).detach().cpu()  \n",
    "        cls_tensor = dsel_embeddings_tensor[:, 0, :]  \n",
    "    \n",
    "        # Convert to NumPy\n",
    "        cls_embeddings = np.ascontiguousarray(cls_tensor.numpy(), dtype='float32')\n",
    "        self.dsel_embeddings = cls_embeddings\n",
    "        self.dsel_labels = torch.cat(dsel_labels).numpy()\n",
    "    \n",
    "        # Build FAISS index\n",
    "        embedding_dim = cls_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(cls_embeddings)\n",
    "\n",
    "\n",
    "    def predict_weighted_robust(self, test_img, k=7, return_logits=False, explain=False, top=False, n=3, \n",
    "                                use_fire=False, per_class_min=1, use_sim=False, sim_threshold=0, \n",
    "                                alpha=0.6, knorae=False):\n",
    "       # Step 1: Get DINO CLS embedding for the test image\n",
    "        img_for_dino = test_img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_emb = self.dino_model.forward_features(img_for_dino).cpu().numpy().astype('float32')\n",
    "            test_emb = test_emb[:, 0, :]  # CLS token only\n",
    "    \n",
    "        # Step 2: Find k nearest neighbors in FAISS\n",
    "        distances, neighbors = self.index.search(test_emb, k)\n",
    "        neighbor_idxs = neighbors[0]\n",
    "        local_labels = self.dsel_labels[neighbor_idxs]\n",
    "        local_labels = np.array(local_labels).flatten()\n",
    "    \n",
    "        # Step 3: Get RoC images\n",
    "        with torch.no_grad():\n",
    "            roc_imgs = torch.stack([self.dsel_dataset[idx][0] for idx in neighbor_idxs]).to(device)\n",
    "    \n",
    "        # Step 4: Evaluate classifiers\n",
    "        competences, soft_outputs, feature_similarities, passed_fire, correct_counts = [], [], [], [], []\n",
    "    \n",
    "        test_img_batch = test_img.unsqueeze(0).to(device)\n",
    "    \n",
    "        for clf in self.pool:\n",
    "            clf.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = clf(roc_imgs)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                correct = (preds == local_labels).sum()\n",
    "                competence = correct / k\n",
    "                competences.append(competence)\n",
    "                correct_counts.append(correct)\n",
    "\n",
    "                # ðŸ”¥ FIRE check\n",
    "                fire_ok = fire_check(local_labels, preds, per_class_min=per_class_min) \n",
    "                passed_fire.append(fire_ok)\n",
    "\n",
    "                logits = clf(test_img.unsqueeze(0).to(device)).squeeze(0)\n",
    "                probs = softmax(logits, dim=0)\n",
    "                soft_outputs.append(probs)\n",
    "\n",
    "                # Feature similarity\n",
    "                test_feat = get_features_before_last_linear(clf, test_img.unsqueeze(0).to(device))\n",
    "                roc_feats = get_features_before_last_linear(clf, roc_imgs)\n",
    "                mean_feat = roc_feats.mean(dim=0, keepdim=True)\n",
    "\n",
    "                test_vec = test_feat.flatten().unsqueeze(0)   # shape (1, D)\n",
    "                mean_vec = mean_feat.flatten().unsqueeze(0)   # shape (1, D)\n",
    "                \n",
    "                sim = cosine_similarity(test_vec, mean_vec)   # shape (1, 1)\n",
    "                feature_similarities.append(sim.item())\n",
    "\n",
    "                # sim = cosine_similarity(test_feat / test_feat.norm(), mean_feat / mean_feat.norm(), dim=1)\n",
    "                # feature_similarities.append(sim.item())\n",
    "\n",
    "        # 5ï¸âƒ£ KNORA-E\n",
    "        if knorae: \n",
    "            selected_indices = []\n",
    "            required_correct = k\n",
    "            while required_correct >= 1 and not selected_indices:\n",
    "                selected_indices = [i for i, c in enumerate(correct_counts) if c >= required_correct]\n",
    "                required_correct -= 1\n",
    "    \n",
    "            if not selected_indices:\n",
    "                selected_indices = list(range(len(self.pool)))\n",
    "\n",
    "            for i in range(len(self.pool)): \n",
    "                if i not in selected_indices: \n",
    "                    competences[i] = 0.0 \n",
    "    \n",
    "        # Step 5: Combine competence & similarity\n",
    "        if use_sim:\n",
    "            selected_feature_sims = [s if s > sim_threshold else 0.0 for s in feature_similarities]\n",
    "            combined_scores = [alpha * c + (1 - alpha) * s for c, s in zip(competences, selected_feature_sims)]\n",
    "        else:\n",
    "            combined_scores = competences[:]\n",
    "        \n",
    "        if use_fire:\n",
    "            combined_scores = [s if passed_fire[i] else 0.0 for i, s in enumerate(combined_scores)]\n",
    "        \n",
    "        # Step 6: Select models\n",
    "        if top:\n",
    "            top_n_idx = np.argsort(combined_scores)[::-1][:n]\n",
    "            total_score = sum(combined_scores[i] for i in top_n_idx)\n",
    "            if total_score == 0:\n",
    "                weights = [1.0 / n] * n\n",
    "            else:\n",
    "                weights = [combined_scores[i] / total_score for i in top_n_idx]\n",
    "        else:\n",
    "            total_score = sum(combined_scores)\n",
    "            if total_score == 0:\n",
    "                weights = [1.0 / len(self.pool)] * len(self.pool)\n",
    "            else:\n",
    "                weights = [s / total_score for s in combined_scores]\n",
    "    \n",
    "        # Step 7: Weighted aggregation\n",
    "        num_classes = 1000\n",
    "        weighted_logits = torch.zeros(num_classes).to(device)\n",
    "\n",
    "        if top: \n",
    "            for idx, weight in zip(top_n_idx, weights):\n",
    "                weighted_logits += weight * soft_outputs[idx]\n",
    "        else: \n",
    "            for prob, weight in zip(soft_outputs, weights):\n",
    "                weighted_logits += weight * prob\n",
    "            \n",
    "        # Step 8: Suspected attacked model\n",
    "        min_sim_idx = int(np.argmin(feature_similarities))\n",
    "        self.suspected_model_votes.append(min_sim_idx)\n",
    "\n",
    "        if return_logits:\n",
    "            return weighted_logits\n",
    "        return weighted_logits.argmax().item()\n",
    "\n",
    "\n",
    "    def get_top_n_competent_models(self, test_img, k=7, top_n=3, use_sim=False, sim_threshold=0, alpha=0.6):\n",
    "        # Step 1: Get DINO CLS embedding for the test image\n",
    "        img_for_dino = test_img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_emb = self.dino_model.forward_features(img_for_dino).cpu().numpy().astype('float32')\n",
    "            test_emb = test_emb[:, 0, :]  # CLS token only\n",
    "    \n",
    "        # Step 2: Find k nearest neighbors in FAISS\n",
    "        distances, neighbors = self.index.search(test_emb, k)\n",
    "        neighbor_idxs = neighbors[0]\n",
    "        local_labels = np.array(self.dsel_labels[neighbor_idxs]).flatten()\n",
    "    \n",
    "        # Step 3: Get RoC images\n",
    "        with torch.no_grad():\n",
    "            roc_imgs = torch.stack([self.dsel_dataset[idx][0] for idx in neighbor_idxs]).to(device)\n",
    "    \n",
    "        # Step 4: Evaluate classifiers\n",
    "        competences, feature_similarities, correct_counts, grad_vectors  = [], [], [], []\n",
    "    \n",
    "        for clf in self.pool:\n",
    "            clf.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = clf(roc_imgs)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                correct = (preds == local_labels).sum()\n",
    "                competences.append(correct / k)\n",
    "                correct_counts.append(correct)\n",
    "    \n",
    "                # Feature similarity\n",
    "                test_feat = get_features_before_last_linear(clf, test_img.unsqueeze(0).to(device))\n",
    "                roc_feats = get_features_before_last_linear(clf, roc_imgs)\n",
    "                mean_feat = roc_feats.mean(dim=0, keepdim=True)\n",
    "                sim = cosine_similarity(test_feat.flatten().unsqueeze(0), mean_feat.flatten().unsqueeze(0))\n",
    "                feature_similarities.append(sim.item())\n",
    "\n",
    "            test_img_req = test_img.clone().detach().unsqueeze(0).to(device)\n",
    "            test_img_req.requires_grad_(True)\n",
    "\n",
    "            out = clf(test_img_req)\n",
    "            pseudo_label = out.argmax(dim=1)  # keep grad path alive\n",
    "            loss = F.cross_entropy(out, pseudo_label)\n",
    "            grad = torch.autograd.grad(loss, test_img_req)[0]\n",
    "            grad_vec = grad.flatten()  # flatten for cosine similarity\n",
    "            grad_vectors.append(grad_vec.cpu())\n",
    "\n",
    "\n",
    "        # Step 5: Compute gradient-based diversity\n",
    "        grad_vectors_tensor = torch.stack(grad_vectors).to(device)  # (num_models, D)\n",
    "        num_models = grad_vectors_tensor.size(0)\n",
    "        \n",
    "        # Compute cosine similarity matrix between gradient vectors\n",
    "        cos_sim_matrix = F.cosine_similarity(\n",
    "            grad_vectors_tensor.unsqueeze(1),  # (num_models, 1, D)\n",
    "            grad_vectors_tensor.unsqueeze(0),  # (1, num_models, D)\n",
    "            dim=-1\n",
    "        )  # (num_models, num_models)\n",
    "        \n",
    "        # Compute \"diversity score\" per model: lower average similarity â†’ more diverse\n",
    "        diversity_scores = 1.0 - cos_sim_matrix.mean(dim=1).cpu().numpy()  # (num_models,)\n",
    "\n",
    "        final_scores = [alpha * c + (1 - alpha) * d for c, d in zip(competences, diversity_scores)] \n",
    "    \n",
    "        # Step 7: Select top_n models\n",
    "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
    "        top_models = [self.pool[i] for i in top_indices]\n",
    "    \n",
    "        return top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e0523-dc2a-482f-a2a8-51ae4f8e1993",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395bbfa7-f6f4-4dd6-813c-69b8721cf1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [05:48<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "des_model = VisionDES_2(val_dataset, ens_models)\n",
    "des_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ee82ed-d916-4f31-8def-f387eef29fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_index = des_model.index\n",
    "saved_dsel_embeddings = des_model.dsel_embeddings \n",
    "saved_dsel_labels = des_model.dsel_labels\n",
    "\n",
    "# des_model.index = saved_index \n",
    "# des_model.dsel_embeddings = saved_dsel_embeddings \n",
    "# des_model.dsel_labels = saved_dsel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb13a78-98e2-4103-b7f4-e0ef46cdbf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d9a2ae-e13d-4018-ae0e-b3feb0e316c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_models = [\n",
    "    get_models(\"imagenet\", \"resnet50\", \"resnet50\"), \n",
    "    get_models(\"imagenet\", \"resnet152\", \"resnet152\"),\n",
    "    get_models(\"imagenet\", \"wide_resnet101_2\", \"wrn101_2\"), \n",
    "    get_models(\"imagenet\", \"resnetv2_50x1_bitm\", \"bit50_1\"),\n",
    "    \n",
    "    get_models(\"imagenet\", \"resnetv2_101x1_bitm\", \"bit101_1\"),\n",
    "    get_models(\"imagenet\", \"vit_base_patch16_224\", \"vit_b\"),\n",
    "    get_models(\"imagenet\", \"deit_base_patch16_224\", \"deit_b\"),\n",
    "    get_models(\"imagenet\", \"swin_base_patch4_window7_224\", \"swin_b\"), \n",
    "    get_models(\"imagenet\", \"swin_small_patch4_window7_224\", \"swin_s\")\n",
    "] \n",
    "\n",
    "des_model.pool = target_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b0c966-f029-4060-873f-c30065c7a5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_labels = []  # <--- collect labels here\n",
    "for idx, (data, label) in enumerate(test_loader):\n",
    "    _, label = data.to(device), label.to(device)\n",
    "    adv_labels.append(label.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75ed736-3750-4ef3-92ce-9c52f4a475f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:14<00:00, 140.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 38.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:32<00:00, 60.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 49.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:23<00:00, 83.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 45.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:19<00:00, 104.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 38.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:34<00:00, 58.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 44.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:13<00:00, 150.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 57.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:14<00:00, 142.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 50.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:37<00:00, 53.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 43.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Sequential on adv: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [00:37<00:00, 53.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Accuracy on adv images: 33.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model in target_models:\n",
    "    model.eval().to(device)\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for adv_img, label in tqdm(zip(adv_images, adv_labels), \n",
    "                                   total=len(adv_images),\n",
    "                                   desc=f\"Evaluating {model.__class__.__name__} on adv\"):\n",
    "            adv_img = adv_img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            if len(adv_img.size()) == 3: \n",
    "                outputs = model(adv_img.unsqueeze(0))\n",
    "            if len(adv_img.size()) == 4: \n",
    "                outputs = model(adv_img) \n",
    "                \n",
    "            if isinstance(outputs, tuple):  # handle models like Inception\n",
    "                outputs = outputs[0]\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print(f\"{model.__class__.__name__} Accuracy on adv images: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74583f-be4c-4bde-8a0f-98c48592099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for adv_img, label in tqdm(zip(adv_images, adv_labels), total=len(adv_images), \n",
    "                               desc=f\"Evaluating VisionDES on adv\"):\n",
    "            adv_img = adv_img.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            outputs = des_model.predict_weighted_robust(adv_img.squeeze(0), k=7, return_logits=False, explain=False, \n",
    "                                                        top=True, n=4, use_fire=False, per_class_min=1, use_sim=True, sim_threshold=0.5, \n",
    "                                                        alpha=0.6, knorae=True)\n",
    "            if isinstance(outputs, tuple):  # handle models like Inception\n",
    "                outputs = outputs[0]\n",
    "            \n",
    "            preds = outputs\n",
    "            correct += (preds == label[0]).sum().item()\n",
    "            total += label.size(0)\n",
    "    \n",
    "    acc = 100 * (correct / total) \n",
    "    print(f\"(VisionDES) Accuracy on adv images: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf043ee-fa63-4d4c-921c-c7db8efeebde",
   "metadata": {},
   "source": [
    "### DEAA -> Ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbda5c9f-d986-426e-a573-c8816d35b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_models = [\n",
    "    get_models(\"imagenet\", \"resnet18\", \"resnet18\"), \n",
    "    get_models(\"imagenet\", \"deit_tiny_patch16_224\", \"deit_t\"),\n",
    "    get_models(\"imagenet\", \"inception_v3\", \"inc_v3\"), \n",
    "    get_models(\"imagenet\", \"vit_tiny_patch16_224\", \"vit_t\"), \n",
    "    get_models(\"imagenet\", \"efficientnet_b0\", \"efficientnet_b0\"), \n",
    "    get_models(\"imagenet\", \"swin_tiny_patch4_window7_224\", \"swin_t\"), \n",
    "    get_models(\"imagenet\", \"xcit_tiny_12_p8_224\", \"swin_t\"), \n",
    "    get_models(\"imagenet\", \"regnetv_040\", \"swin_t\"), \n",
    "] \n",
    "\n",
    "des_model.pool = ens_models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acfbefa-76eb-4e9f-b2a0-74d3ca71ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2004/2004 [1:24:36<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "adv_images = []     \n",
    "adv_labels = []\n",
    "\n",
    "eps = 8/255\n",
    "alpha = 2/255\n",
    "iters = 10\n",
    "\n",
    "\n",
    "for data, label in tqdm(test_loader):\n",
    "    data, label = data.to(device), label.to(device)\n",
    "\n",
    "    # ðŸ™ Please work \n",
    "    selected_models = des_model.get_top_n_competent_models(\n",
    "            data[0],                    \n",
    "            k=7,\n",
    "            top_n=4,\n",
    "            use_sim=False,\n",
    "            sim_threshold=0,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "    adv = ensemble_ifgsm(selected_models, data, label, eps=eps, alpha=alpha, iters=iters, clip_min=0.0, clip_max=1.0)\n",
    "    \n",
    "    adv_images.append(adv.detach().cpu())\n",
    "    adv_labels.append(label.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35f022-59a6-47ba-bcbe-f993c1083d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
