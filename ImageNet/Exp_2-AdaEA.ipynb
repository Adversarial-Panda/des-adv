{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d97e53-575d-4ea2-bef7-3ae4bc071ae7",
   "metadata": {},
   "source": [
    "### Experiment 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4e2112-9295-4fc3-a413-4f581236a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets \n",
    "from torch.utils.data import DataLoader\n",
    "# from medmnist import INFO\n",
    "import numpy as np\n",
    "import faiss\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.functional import softmax, cosine_similarity\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa82cad-2f14-41fd-af2d-a98e0071b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a18a973-e35f-4969-b9c3-d25adfa062ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined class mapping built: 1000 total classes\n",
      "✅ Loaded 1000 hard samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, class_to_idx, transform=None):\n",
    "        with open(file_list, \"r\") as f:\n",
    "            self.samples = [line.strip() for line in f]\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        class_folder = os.path.basename(os.path.dirname(img_path))\n",
    "        label = self.class_to_idx.get(class_folder, -1)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# ---------------- Create a combined class mapping ----------------\n",
    "root_dir = \"dataset/imagenet_tests\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Collect class mappings from all 10 partitions\n",
    "combined_class_to_idx = {}\n",
    "for i in range(1, 11):\n",
    "    test_dir = os.path.join(root_dir, f\"test{i}\")\n",
    "    dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    combined_class_to_idx.update(dataset.class_to_idx)\n",
    "\n",
    "print(f\"✅ Combined class mapping built: {len(combined_class_to_idx)} total classes\")\n",
    "\n",
    "# ---------------- Load your 1000-image subset ----------------\n",
    "subset_file = \"results/hard_cases_missed_by_mobilenet.txt\"\n",
    "hard_dataset = CustomImageListDataset(subset_file, class_to_idx=combined_class_to_idx, transform=transform)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"✅ Loaded {len(hard_dataset)} hard samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95a897f-a3ef-44cf-b610-768de96480f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(dataset, model_name, key): \n",
    "    if dataset == 'imagenet':\n",
    "        # save_root_path = r\"checkpoint/tinyimagenet\"\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=1000).to(device)\n",
    "        model.eval()\n",
    "        if 'inc' in key or 'vit' in key or 'bit' in key:\n",
    "            return torch.nn.Sequential(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), model)\n",
    "        else:\n",
    "            return torch.nn.Sequential(transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede37b1f-d656-4161-a08b-2f055ff17ae6",
   "metadata": {},
   "source": [
    "### Ensemble Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25803b27-2066-4640-a2a9-71323efceb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AdaEA_Base:\n",
    "    def __init__(self, models, eps=8/255, alpha=2/255, max_value=1., min_value=0., threshold=0., beta=10,\n",
    "                 device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')):\n",
    "        assert isinstance(models, list) and len(models) >= 2, 'Error'\n",
    "        self.device = device\n",
    "        self.models = models\n",
    "        self.num_models = len(self.models)\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "\n",
    "        # attack parameter\n",
    "        self.eps = eps\n",
    "        self.threshold = threshold\n",
    "        self.max_value = max_value\n",
    "        self.min_value = min_value\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def get_adv_example(self, ori_data, adv_data, grad, attack_step=None):\n",
    "        \"\"\"\n",
    "        :param ori_data: original image\n",
    "        :param adv_data: adversarial image in the last iteration\n",
    "        :param grad: gradient in this iteration\n",
    "        :return: adversarial example in this iteration\n",
    "        \"\"\"\n",
    "        if attack_step is None:\n",
    "            adv_example = adv_data.detach() + grad.sign() * self.alpha\n",
    "        else:\n",
    "            adv_example = adv_data.detach() + grad.sign() * attack_step\n",
    "        delta = torch.clamp(adv_example - ori_data.detach(), -self.eps, self.eps)\n",
    "        return torch.clamp(ori_data.detach() + delta, max=self.max_value, min=self.min_value)\n",
    "\n",
    "    def agm(self, ori_data, cur_adv, grad, label):\n",
    "        \"\"\"\n",
    "        Adaptive gradient modulation\n",
    "        :param ori_data: natural images\n",
    "        :param cur_adv: adv examples in last iteration\n",
    "        :param grad: gradient in this iteration\n",
    "        :param label: ground truth\n",
    "        :return: coefficient of each model\n",
    "        \"\"\"\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # generate adversarial example\n",
    "        adv_exp = [self.get_adv_example(ori_data=ori_data, adv_data=cur_adv, grad=grad[idx])\n",
    "                   for idx in range(self.num_models)]\n",
    "        loss_self = [loss_func(self.models[idx](adv_exp[idx]), label) for idx in range(self.num_models)]\n",
    "        w = torch.zeros(size=(self.num_models,), device=self.device)\n",
    "\n",
    "        for j in range(self.num_models):\n",
    "            for i in range(self.num_models):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                w[j] += loss_func(self.models[i](adv_exp[j]), label) / loss_self[i] * self.beta\n",
    "        w = torch.softmax(w, dim=0)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def drf(self, grads, data_size):\n",
    "        \"\"\"\n",
    "        disparity-reduced filter\n",
    "        :param grads: gradients of each model\n",
    "        :param data_size: size of input images\n",
    "        :return: reduce map\n",
    "        \"\"\"\n",
    "        reduce_map = torch.zeros(size=(self.num_models, self.num_models, data_size[0], data_size[-2], data_size[-1]),\n",
    "                                 dtype=torch.float, device=self.device)\n",
    "        sim_func = torch.nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "        reduce_map_result = torch.zeros(size=(self.num_models, data_size[0], data_size[-2], data_size[-1]),\n",
    "                                        dtype=torch.float, device=self.device)\n",
    "        for i in range(self.num_models):\n",
    "            for j in range(self.num_models):\n",
    "                if i >= j:\n",
    "                    continue\n",
    "                reduce_map[i][j] = sim_func(F.normalize(grads[i], dim=1), F.normalize(grads[j], dim=1))\n",
    "            if i < j:\n",
    "                one_reduce_map = (reduce_map[i, :].sum(dim=0) + reduce_map[:, i].sum(dim=0)) / (self.num_models - 1)\n",
    "                reduce_map_result[i] = one_reduce_map\n",
    "\n",
    "        return reduce_map_result.mean(dim=0).view(data_size[0], 1, data_size[-2], data_size[-1])\n",
    "\n",
    "    @abstractmethod\n",
    "    def attack(self,\n",
    "               data: torch.Tensor,\n",
    "               label: torch.Tensor,\n",
    "               idx: int = -1) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "    def __call__(self, data, label, idx=-1):\n",
    "        return self.attack(data, label, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ebcd5b-5d59-430c-b638-a538c0ccb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdaEA_MIFGSM(AdaEA_Base):\n",
    "    def __init__(self, models, eps=8/255, alpha=2/255, iters=20, max_value=1., min_value=0., threshold=0., beta=10,\n",
    "                 device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'), momentum=0.9):\n",
    "        super().__init__(models=models, eps=eps, alpha=alpha, max_value=max_value, min_value=min_value,\n",
    "                         threshold=threshold, device=device, beta=beta)\n",
    "        self.iters = iters\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def attack(self, data, label, idx=-1):\n",
    "        B, C, H, W = data.size()\n",
    "        data = data.clone().detach().to(self.device)\n",
    "        label = label.clone().detach().to(self.device)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # init pert\n",
    "        adv_data = data.clone().detach() + 0.001 * torch.randn(data.shape, device=self.device)\n",
    "        adv_data = adv_data.detach()\n",
    "\n",
    "        grad_mom = torch.zeros_like(data, device=self.device)\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            adv_data.requires_grad = True\n",
    "\n",
    "            outputs = [self.models[idx](adv_data) for idx in range(len(self.models))]\n",
    "            losses = [loss_func(outputs[idx], label) for idx in range(len(self.models))]\n",
    "            grads = [torch.autograd.grad(losses[idx], adv_data, retain_graph=True, create_graph=False)[0]\n",
    "                     for idx in range(len(self.models))]\n",
    "\n",
    "            # AGM\n",
    "            alpha = self.agm(ori_data=data, cur_adv=adv_data, grad=grads, label=label)\n",
    "\n",
    "            # DRF\n",
    "            cos_res = self.drf(grads, data_size=(B, C, H, W))\n",
    "            cos_res[cos_res >= self.threshold] = 1.\n",
    "            cos_res[cos_res < self.threshold] = 0.\n",
    "\n",
    "            output = torch.stack(outputs, dim=0) * alpha.view(self.num_models, 1, 1)\n",
    "            output = output.sum(dim=0)\n",
    "            loss = loss_func(output, label)\n",
    "            grad = torch.autograd.grad(loss.sum(dim=0), adv_data)[0]\n",
    "            grad = grad * cos_res\n",
    "\n",
    "            # momentum\n",
    "            grad = grad / torch.mean(torch.abs(grad), dim=(1, 2, 3), keepdim=True)\n",
    "            grad = grad + self.momentum * grad_mom\n",
    "            grad_mom = grad\n",
    "\n",
    "            # add perturbation\n",
    "            adv_data = self.get_adv_example(ori_data=data, adv_data=adv_data, grad=grad)\n",
    "            adv_data.detach_()\n",
    "\n",
    "        return adv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c193303-4d37-4841-b307-cfa155e0c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AdaEA_PGD(AdaEA_Base):\n",
    "    def __init__(self, models, eps=8/255, alpha=2/255, iters=20, max_value=1., min_value=0., threshold=0.,\n",
    "                 beta=10, device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "                 random_start=False):\n",
    "        super().__init__(models=models, eps=eps, alpha=alpha, max_value=max_value, min_value=min_value,\n",
    "                         threshold=threshold, device=device, beta=beta)\n",
    "        self.iters = iters\n",
    "        self.random_start = random_start\n",
    "\n",
    "    def attack(self, data, label, idx=-1):\n",
    "        \"\"\"\n",
    "        AdaEA PGD attack — same AGM + DRF + ensemble averaging logic as AdaEA_MIFGSM,\n",
    "        but with PGD (no momentum).\n",
    "        \"\"\"\n",
    "        B, C, H, W = data.size()\n",
    "        data = data.clone().detach().to(self.device)\n",
    "        label = label.clone().detach().to(self.device)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # init pert: either small gaussian (like MIFGSM) or a uniform random start within eps-ball\n",
    "        if self.random_start:\n",
    "            adv_data = data.clone().detach() + torch.empty_like(data).uniform_(-self.eps, self.eps)\n",
    "            adv_data = torch.clamp(adv_data, min=self.min_value, max=self.max_value).detach()\n",
    "        else:\n",
    "            adv_data = data.clone().detach() + 0.001 * torch.randn(data.shape, device=self.device)\n",
    "            adv_data = adv_data.detach()\n",
    "\n",
    "        for i in range(self.iters):\n",
    "            adv_data.requires_grad = True\n",
    "\n",
    "            # forward each model on current adversarial example\n",
    "            outputs = [self.models[m_idx](adv_data) for m_idx in range(len(self.models))]\n",
    "            losses = [loss_func(outputs[m_idx], label) for m_idx in range(len(self.models))]\n",
    "\n",
    "            # per-model gradient (for AGM)\n",
    "            grads = [torch.autograd.grad(losses[m_idx], adv_data, retain_graph=True, create_graph=False)[0]\n",
    "                     for m_idx in range(len(self.models))]\n",
    "\n",
    "            # AGM: obtain per-model coefficients w (shape: num_models,)\n",
    "            alpha_coeffs = self.agm(ori_data=data, cur_adv=adv_data, grad=grads, label=label)\n",
    "\n",
    "            # DRF: cosine similarity based reduce map\n",
    "            cos_res = self.drf(grads, data_size=(B, C, H, W))\n",
    "            cos_res[cos_res >= self.threshold] = 1.\n",
    "            cos_res[cos_res < self.threshold] = 0.\n",
    "\n",
    "            # ensemble-weighted logits like in MIFGSM\n",
    "            output = torch.stack(outputs, dim=0) * alpha_coeffs.view(self.num_models, 1, 1)\n",
    "            output = output.sum(dim=0)\n",
    "            loss = loss_func(output, label)\n",
    "\n",
    "            # compute gradient of ensemble loss\n",
    "            grad = torch.autograd.grad(loss.sum(dim=0), adv_data)[0]\n",
    "\n",
    "            # apply DRF mask\n",
    "            grad = grad * cos_res\n",
    "\n",
    "            # normalization (same style as your MIFGSM: per-sample mean absolute normalization)\n",
    "            grad = grad / (torch.mean(torch.abs(grad), dim=(1, 2, 3), keepdim=True) + 1e-12)\n",
    "\n",
    "            # PGD step: sign-based step (keeps get_adv_example semantics)\n",
    "            adv_data = self.get_adv_example(ori_data=data, adv_data=adv_data, grad=grad)\n",
    "            adv_data.detach_()\n",
    "\n",
    "        return adv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "248a03f6-a962-46b6-916c-39cc24358bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaEA_TIDIM(AdaEA_Base):\n",
    "    def __init__(self,\n",
    "                 models,\n",
    "                 eps=8/255,\n",
    "                 alpha=2/255,\n",
    "                 iters=10,\n",
    "                 max_value=1.,\n",
    "                 min_value=0.,\n",
    "                 threshold=0.,\n",
    "                 beta=10,\n",
    "                 device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n",
    "                 momentum=1.0,\n",
    "                 kernel_size=5,\n",
    "                 resize_prob=0.5,\n",
    "                 diversity_scale=0.12,\n",
    "                 n_eot=1):\n",
    "        \"\"\"\n",
    "        TI-DIM: Translation-Invariant + Diverse-Input + Momentum Iterative FGSM\n",
    "        n_eot: number of EOT samples per iteration (1 is standard; >1 improves stability)\n",
    "        \"\"\"\n",
    "        super().__init__(models=models, eps=eps, alpha=alpha,\n",
    "                         max_value=max_value, min_value=min_value,\n",
    "                         threshold=threshold, device=device, beta=beta)\n",
    "\n",
    "        self.iters = iters\n",
    "        self.momentum = momentum\n",
    "        self.kernel_size = kernel_size\n",
    "        self.resize_prob = resize_prob\n",
    "        self.diversity_scale = diversity_scale\n",
    "        self.n_eot = n_eot\n",
    "\n",
    "        # create gaussian TI kernel (1,1,k,k), we'll expand it per-channel in TI_gradient\n",
    "        self.kernel = self._make_gaussian_kernel(kernel_size, sigma=kernel_size / 3.0).to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_gaussian_kernel(kernel_size=5, sigma=1.0):\n",
    "        ax = torch.arange(-(kernel_size // 2), kernel_size // 2 + 1, dtype=torch.float32)\n",
    "        xx, yy = torch.meshgrid(ax, ax, indexing='xy')\n",
    "        kernel = torch.exp(-(xx ** 2 + yy ** 2) / (2.0 * sigma ** 2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "        return kernel\n",
    "\n",
    "    def DI_transform(self, x):\n",
    "        \"\"\"\n",
    "        Diverse Input transform:\n",
    "        - with probability `resize_prob` randomly resize to rnd in [H*(1-s), H] then pad/crop back\n",
    "        - uses torch RNG for reproducibility\n",
    "        \"\"\"\n",
    "        if torch.rand(1).item() > self.resize_prob:\n",
    "            return x\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        s = float(self.diversity_scale)\n",
    "        rnd = int(H * (1.0 - s) + torch.rand(1).item() * H * s)\n",
    "        rnd = max(1, rnd)\n",
    "\n",
    "        x_resized = F.interpolate(x, size=(rnd, rnd), mode='bilinear', align_corners=False)\n",
    "\n",
    "        if rnd < H:\n",
    "            pad_h = H - rnd\n",
    "            pad_w = W - rnd\n",
    "            pad_top = pad_h // 2\n",
    "            pad_left = pad_w // 2\n",
    "            pad_bottom = pad_h - pad_top\n",
    "            pad_right = pad_w - pad_left\n",
    "            x_padded = F.pad(x_resized, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0.0)\n",
    "            return x_padded\n",
    "        elif rnd > H:\n",
    "            # center-crop back\n",
    "            start = (rnd - H) // 2\n",
    "            return x_resized[:, :, start:start + H, start:start + W]\n",
    "        else:\n",
    "            return x_resized\n",
    "\n",
    "    def TI_gradient(self, grad):\n",
    "        \"\"\"\n",
    "        Apply translation-invariant smoothing to gradient using grouped conv.\n",
    "        grad: (B,C,H,W)\n",
    "        returns smoothed grad of same shape\n",
    "        \"\"\"\n",
    "        B, C, H, W = grad.shape\n",
    "        # kernel is (1,1,k,k) -> expand to (C,1,k,k) for grouped conv\n",
    "        kernel = self.kernel.to(grad.device).type(grad.dtype)\n",
    "        if kernel.shape[0] == 1:\n",
    "            kernel = kernel.repeat(C, 1, 1, 1)\n",
    "        # grouped conv\n",
    "        pad = kernel.shape[-1] // 2\n",
    "        grad_smooth = F.conv2d(grad, weight=kernel, bias=None, stride=1, padding=pad, groups=C)\n",
    "        return grad_smooth\n",
    "\n",
    "    def attack(self, data, label, idx=-1):\n",
    "        \"\"\"\n",
    "        TI-DIM attack:\n",
    "        - Uses AGM to compute per-model weights and averages logits with those weights\n",
    "        - Applies DI transform, TI smoothing, momentum, and MI-FGSM updates\n",
    "        \"\"\"\n",
    "        B, C, H, W = data.size()\n",
    "        data = data.clone().detach().to(self.device)\n",
    "        label = label.clone().detach().to(self.device)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        # initialization\n",
    "        adv_data = data.clone().detach() + 0.001 * torch.randn_like(data).to(self.device)\n",
    "        adv_data = adv_data.detach()\n",
    "        grad_mom = torch.zeros_like(data).to(self.device)\n",
    "\n",
    "        step_size = self.alpha if hasattr(self, 'alpha') and self.alpha is not None else (self.eps / float(self.iters))\n",
    "        eps_small = 1e-12\n",
    "\n",
    "        for it in range(self.iters):\n",
    "            # EOT accumulation (average over n_eot DI samples)\n",
    "            grad_eot_accum = torch.zeros_like(adv_data).to(self.device)\n",
    "\n",
    "            # We'll also collect per-model grads for AGM/DRF if needed (useful for agm/drf)\n",
    "            per_model_grads_for_agm = [torch.zeros_like(adv_data).to(self.device) for _ in range(self.num_models)]\n",
    "\n",
    "            for e in range(self.n_eot):\n",
    "                adv_data.requires_grad_(True)\n",
    "\n",
    "                # apply DI\n",
    "                adv_DI = self.DI_transform(adv_data)\n",
    "\n",
    "                # forward each model to get logits\n",
    "                logits_list = []\n",
    "                for m in self.models:\n",
    "                    out = m(adv_DI)\n",
    "                    if isinstance(out, (tuple, list)):\n",
    "                        out = out[0]\n",
    "                    logits_list.append(out)\n",
    "\n",
    "                # compute per-model losses (for AGM weighting)\n",
    "                losses_per_model = [loss_func(logits_list[j], label) for j in range(self.num_models)]\n",
    "\n",
    "                # compute per-model grads (w.r.t adv_data) if AGM or DRF needs them\n",
    "                grads = [torch.autograd.grad(losses_per_model[j], adv_data, retain_graph=True, create_graph=False)[0]\n",
    "                         for j in range(self.num_models)]\n",
    "\n",
    "                # keep grads for agm/drf outside EOT loop (we average later)\n",
    "                for j in range(self.num_models):\n",
    "                    per_model_grads_for_agm[j] += grads[j].detach()\n",
    "\n",
    "                # compute AGM weights (use detached grads for stability). AGM expects grads list\n",
    "                try:\n",
    "                    agm_w = self.agm(ori_data=data, cur_adv=adv_data.detach(), grad=[g.detach() for g in grads], label=label)\n",
    "                    # ensure shape (num_models,)\n",
    "                    if agm_w is None:\n",
    "                        agm_w = torch.ones(self.num_models, device=self.device) / float(self.num_models)\n",
    "                except Exception:\n",
    "                    agm_w = torch.ones(self.num_models, device=self.device) / float(self.num_models)\n",
    "\n",
    "                # apply AGM weights on logits: weighted sum of logits\n",
    "                # logits_list is list of (B, num_classes)\n",
    "                stacked_logits = torch.stack(logits_list, dim=0)  # (num_models, B, C_out)\n",
    "                # agm_w: (num_models,) -> (num_models, 1, 1) to broadcast\n",
    "                wview = agm_w.view(self.num_models, 1, 1)\n",
    "                weighted_logits = (stacked_logits * wview).sum(dim=0)  # (B, C_out)\n",
    "\n",
    "                # combined loss from weighted logits\n",
    "                loss_combined = loss_func(weighted_logits, label)\n",
    "                # gradient of combined loss w.r.t adv_data\n",
    "                grad_combined = torch.autograd.grad(loss_combined, adv_data, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "                grad_eot_accum += grad_combined.detach()\n",
    "\n",
    "                # free grads for next eot iteration\n",
    "                adv_data.grad = None\n",
    "                for m in self.models:\n",
    "                    if hasattr(m, 'zero_grad'):\n",
    "                        m.zero_grad()\n",
    "\n",
    "            # average over EOT samples\n",
    "            grad_avg = grad_eot_accum / float(self.n_eot)\n",
    "\n",
    "            # compute DRF mask from averaged per-model grads if possible\n",
    "            try:\n",
    "                per_model_grads_avg = [g / float(self.n_eot) for g in per_model_grads_for_agm]\n",
    "                cos_res = self.drf(per_model_grads_avg, data_size=(B, C, H, W))\n",
    "                # thresholding\n",
    "                cos_res = (cos_res >= self.threshold).float().to(self.device)\n",
    "                # broadcast if necessary\n",
    "                if cos_res.shape != grad_avg.shape:\n",
    "                    # attempt to broadcast: (B,1,H,W) -> (B,C,H,W)\n",
    "                    if cos_res.dim() == 4 and cos_res.size(1) == 1:\n",
    "                        cos_res = cos_res.repeat(1, C, 1, 1)\n",
    "                grad_avg = grad_avg * cos_res\n",
    "            except Exception:\n",
    "                # if drf fails, continue without mask\n",
    "                pass\n",
    "\n",
    "            # TI smoothing\n",
    "            grad_ti = self.TI_gradient(grad_avg)\n",
    "\n",
    "            # normalize per-sample\n",
    "            denom = torch.mean(torch.abs(grad_ti), dim=(1, 2, 3), keepdim=True).clamp(min=eps_small)\n",
    "            grad_norm = grad_ti / denom\n",
    "\n",
    "            # momentum update\n",
    "            grad_mom = self.momentum * grad_mom + grad_norm\n",
    "\n",
    "            # MI-FGSM update (sign of momentum)\n",
    "            adv_data = adv_data.detach() + step_size * torch.sign(grad_mom)\n",
    "\n",
    "            # clip to eps ball and valid range\n",
    "            adv_data = torch.max(torch.min(adv_data, data + self.eps), data - self.eps)\n",
    "            adv_data = torch.clamp(adv_data, min=self.min_value, max=self.max_value).detach()\n",
    "\n",
    "            # zero model grads\n",
    "            for m in self.models:\n",
    "                if hasattr(m, 'zero_grad'):\n",
    "                    m.zero_grad()\n",
    "\n",
    "        return adv_data.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926452c-4ce0-43e0-87b3-fba472b6b367",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dc5ceb8-c970-448a-a862-7d64f3e37476",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_models = [\n",
    "    get_models(\"imagenet\", \"resnet18\", \"resnet18\"), \n",
    "    get_models(\"imagenet\", \"inception_v3\", \"inc_v3\"), \n",
    "    get_models(\"imagenet\", \"deit_tiny_patch16_224\", \"deit_t\"),\n",
    "    get_models(\"imagenet\", \"vit_tiny_patch16_224\", \"vit_t\"), \n",
    "    # get_models(\"imagenet\", \"efficientnet_b0\", \"efficientnet_b0\"), \n",
    "    # get_models(\"imagenet\", \"xcit_tiny_12_p8_224\", \"swin_t\"), \n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1292f8e0-0054-41c2-b0dd-f32c3072aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = 1.0 \n",
    "min_value = 0.0 \n",
    "eps = 8/255\n",
    "alpha = 2/255 \n",
    "iters = 10 \n",
    "threshold = -0.3\n",
    "beta = 10 \n",
    "\n",
    "# attack_method = AdaEA_MIFGSM(ens_models, eps=eps, alpha=alpha, iters=iters, max_value=max_value, \n",
    "#                             min_value=min_value, beta=beta, threshold=threshold, device=device)\n",
    "\n",
    "# attack_method = AdaEA_PGD(ens_models, eps=eps, alpha=alpha, iters=iters, max_value=max_value, \n",
    "#                             min_value=min_value, beta=beta, threshold=threshold, device=device)\n",
    "\n",
    "\n",
    "\n",
    "attack_method = AdaEA_TIDIM(ens_models, eps=16/255, alpha=2/255, iters=iters, kernel_size=15,  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a09d938a-a7be-4bdf-a094-df94b31cd6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating MI-FGSM adversarials (GPU): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [57:34<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 1000 adversarial images. Shape: torch.Size([1000, 3, 224, 224])\n",
      "Noise (1 - SSIM): mean=0.169276, std=0.070937, min=0.000290, max=0.539290\n",
      "Mean absolute pixel diff (after clamp to [0,1]): mean=0.044744, std=0.003677\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional.image import structural_similarity_index_measure as ssim\n",
    "import torch\n",
    "\n",
    "# --- before loop (clear previous lists) ---\n",
    "adv_list = []\n",
    "orig_list = []\n",
    "labels_list = []\n",
    "noise_rates = []\n",
    "pixel_diffs = []\n",
    "\n",
    "def ensure_batch(x):\n",
    "    return x if x.dim() == 4 else x.unsqueeze(0)\n",
    "\n",
    "def to_unit_range(x):\n",
    "    \"\"\"\n",
    "    Ensure x is in [0,1]. If tensor values appear to be in [0,255] (max>1.5),\n",
    "    convert by dividing by 255. Returns a float tensor on same device.\n",
    "    \"\"\"\n",
    "    x = ensure_batch(x).float()\n",
    "    if x.max().item() > 1.5:\n",
    "        x = x / 255.0\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "# --- attack loop (same as yours, but using the simplified functions) ---\n",
    "for img, label in tqdm(hard_loader, desc=\"Generating MI-FGSM adversarials (GPU)\"):\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        adv_img = attack_method(img, label) \n",
    "\n",
    "    # store for later (move to CPU)\n",
    "    adv_list.append(adv_img.squeeze(0).cpu())\n",
    "    orig_list.append(img.squeeze(0).cpu())\n",
    "    labels_list.append(label.squeeze(0).cpu())\n",
    "\n",
    "    # compute SSIM and pixel diffs on [0,1] images\n",
    "    img_for_ssim = to_unit_range(img)       # (1,C,H,W) in [0,1]\n",
    "    adv_for_ssim = to_unit_range(adv_img)   # (1,C,H,W) in [0,1]\n",
    "\n",
    "    ssim_val = ssim(adv_for_ssim, img_for_ssim)  # scalar tensor\n",
    "    noise_rates.append((1.0 - float(ssim_val)))\n",
    "    pixel_diffs.append((adv_for_ssim - img_for_ssim).abs().mean().item())\n",
    "\n",
    "# --- stack everything on CPU ---\n",
    "adv_all = torch.stack(adv_list).cpu()\n",
    "orig_all = torch.stack(orig_list).cpu()\n",
    "labels_all = torch.stack(labels_list).cpu()\n",
    "\n",
    "noise_rates = torch.tensor(noise_rates)\n",
    "pixel_diffs = torch.tensor(pixel_diffs)\n",
    "\n",
    "print(f\"✅ Generated {adv_all.size(0)} adversarial images. Shape: {adv_all.shape}\")\n",
    "print(f\"Noise (1 - SSIM): mean={noise_rates.mean():.6f}, std={noise_rates.std():.6f}, min={noise_rates.min():.6f}, max={noise_rates.max():.6f}\")\n",
    "print(f\"Mean absolute pixel diff (after clamp to [0,1]): mean={pixel_diffs.mean():.6f}, std={pixel_diffs.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07126e7e-6959-413f-9db6-0ee8a2bdde71",
   "metadata": {},
   "source": [
    "### Test on Target models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da3aed9c-b566-4a86-bfce-1ce88a0ef904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32  # tune this for your GPU\n",
    "dataset = TensorDataset(adv_all, labels_all)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72d1e1b0-e8a7-4241-b295-40bf6eb7e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_models = [\n",
    "    get_models(\"imagenet\", \"resnet152\", \"resnet152\"),\n",
    "    get_models(\"imagenet\", \"wide_resnet101_2\", \"wrn101_2\"),     \n",
    "    get_models(\"imagenet\", \"regnety_320\", \"regnety_320\"),\n",
    "    get_models(\"imagenet\", \"vgg19\", \"vgg19\"),\n",
    "    get_models(\"imagenet\", \"vit_base_patch16_224\", \"vit_b\"),\n",
    "    get_models(\"imagenet\", \"deit_base_patch16_224\", \"deit_b\"),\n",
    "    # get_models(\"imagenet\", \"swin_base_patch4_window7_224\", \"swin_b\"), \n",
    "    # get_models(\"imagenet\", \"mixer_b16_224\", \"vit_t\"), \n",
    "    # get_models(\"imagenet\", \"convmixer_768_32\", \"vit_t\")\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c36ce32e-6f58-49c5-86f4-ec8c013333d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 21.60%  (216/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 54.70%  (547/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:10<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 20.20%  (202/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 64.10%  (641/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 17.70%  (177/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:04<00:00,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 34.60%  (346/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for t_model in target_models:\n",
    "        name = getattr(t_model, \"name\", t_model.__class__.__name__)\n",
    "        t_model.eval()\n",
    "        t_model.to(device)\n",
    "\n",
    "        fooled = 0\n",
    "        total = 0\n",
    "\n",
    "        for imgs_cpu, labels_cpu in tqdm(loader, desc=f\"ASR {name}\"):\n",
    "            # Move to device here\n",
    "            imgs = imgs_cpu.to(device, non_blocking=True)\n",
    "            labels = labels_cpu.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = t_model(imgs)\n",
    "            if isinstance(outputs, (tuple, list)):\n",
    "                outputs = outputs[0]\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            fooled += (preds != labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # free cache per batch (helps on tight GPUs)\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        asr = 100.0 * fooled / total if total > 0 else 0.0\n",
    "        print(f\"{name}: ASR = {asr:.2f}%  ({fooled}/{total} fooled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91655f0f-2201-4fb5-be4f-6f5bdb49336a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
