{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6326352-34c1-4501-816c-69dd1b01e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets \n",
    "from torch.utils.data import DataLoader\n",
    "# from medmnist import INFO\n",
    "import numpy as np\n",
    "import faiss\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.nn.functional import softmax, cosine_similarity\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f78c7d5-905a-45b7-8790-88e731847d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ccc99d-dfbe-4d4e-8035-4a4881734e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined class mapping built: 1000 total classes\n",
      "âœ… Loaded 1000 hard samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageListDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_list, class_to_idx, transform=None):\n",
    "        with open(file_list, \"r\") as f:\n",
    "            self.samples = [line.strip() for line in f]\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        class_folder = os.path.basename(os.path.dirname(img_path))\n",
    "        label = self.class_to_idx.get(class_folder, -1)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# ---------------- Create a combined class mapping ----------------\n",
    "root_dir = \"dataset/imagenet_tests\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Collect class mappings from all 10 partitions\n",
    "combined_class_to_idx = {}\n",
    "for i in range(1, 11):\n",
    "    test_dir = os.path.join(root_dir, f\"test{i}\")\n",
    "    dataset = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    combined_class_to_idx.update(dataset.class_to_idx)\n",
    "\n",
    "print(f\"âœ… Combined class mapping built: {len(combined_class_to_idx)} total classes\")\n",
    "\n",
    "# ---------------- Load your 1000-image subset ----------------\n",
    "subset_file = \"results/hard_cases_missed_by_mobilenet.txt\"\n",
    "hard_dataset = CustomImageListDataset(subset_file, class_to_idx=combined_class_to_idx, transform=transform)\n",
    "hard_loader = DataLoader(hard_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"âœ… Loaded {len(hard_dataset)} hard samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4313ac4d-e778-4bf5-b7e2-73fd261a4d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading dataset with resize transform...\n",
      "Validation samples: 30000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "print(f\"Step 1: Loading dataset with resize transform...\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_data_dir = 'dataset/imagenet_validation' \n",
    "\n",
    "val_dataset = datasets.ImageFolder(os.path.join(val_data_dir), transform=transform) \n",
    "\n",
    "\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ff8bcf-7df8-411a-ba9a-042ab6018d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(dataset, model_name, key): \n",
    "    if dataset == 'imagenet':\n",
    "        # save_root_path = r\"checkpoint/tinyimagenet\"\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=1000).to(device)\n",
    "        model.eval()\n",
    "        if 'inc' in key or 'vit' in key or 'bit' in key:\n",
    "            return torch.nn.Sequential(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), model)\n",
    "        else:\n",
    "            return torch.nn.Sequential(transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06190d5-5cd0-499a-8a51-ef79f4ff5f29",
   "metadata": {},
   "source": [
    "### Ensemble Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8851369-830e-4cbb-8cb6-02676a374761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "def ensemble_mi_fgsm(\n",
    "    models: List[torch.nn.Module],\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    eps: float = 8/255,\n",
    "    alpha: float = 2/255,\n",
    "    iters: int = 10,\n",
    "    decay: float = 1.0,\n",
    "    clip_min: float = 0.0,\n",
    "    clip_max: float = 1.0,\n",
    "    loss_fn=None,\n",
    "    device: str = None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "\n",
    "    if not isinstance(models, (list, tuple)):\n",
    "        models = [models]\n",
    "\n",
    "    for m in models:\n",
    "        m.to(device).eval()\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    x_orig = x.clone().detach().to(device).float()\n",
    "    x_adv = x_orig.clone().detach()\n",
    "    momentum = torch.zeros_like(x_adv).to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "        # ----- Liu et al. (2017): sum/average logits before loss -----\n",
    "        sum_logits = None\n",
    "        for m in models:\n",
    "            out = m(x_adv)\n",
    "            if isinstance(out, (tuple, list)):\n",
    "                out = out[0]\n",
    "            sum_logits = out if sum_logits is None else sum_logits + out\n",
    "        avg_logits = sum_logits / len(models)\n",
    "        total_loss = loss_fn(avg_logits, y)\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        grad = torch.autograd.grad(total_loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        grad = grad / (torch.norm(grad, p=1) + 1e-8)\n",
    "\n",
    "        # momentum update (MI-FGSM)\n",
    "        momentum = decay * momentum + grad\n",
    "        step = alpha * momentum.sign()\n",
    "\n",
    "        x_adv = x_adv.detach() + step.detach()\n",
    "        delta = torch.clamp(x_adv - x_orig, min=-eps, max=eps)\n",
    "        x_adv = torch.clamp(x_orig + delta, min=clip_min, max=clip_max).detach()\n",
    "\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3132a321-9cf4-4b80-aeab-b7815e3cf6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "def ensemble_pgd(\n",
    "    models: List[torch.nn.Module],\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    eps: float = 8/255,\n",
    "    alpha: float = 2/255,\n",
    "    iters: int = 10,\n",
    "    clip_min: float = 0.0,\n",
    "    clip_max: float = 1.0,\n",
    "    loss_fn=None,\n",
    "    device: str = None,\n",
    "    random_start: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensemble PGD (average-logits) â€” same ensemble formulation as your MI-FGSM function\n",
    "    but using PGD (no momentum).\n",
    "    - models: list of models (or single model)\n",
    "    - random_start: if True, initialize x_adv with a random perturbation within the eps-ball\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = x.device\n",
    "\n",
    "    if not isinstance(models, (list, tuple)):\n",
    "        models = [models]\n",
    "\n",
    "    for m in models:\n",
    "        m.to(device).eval()\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    x_orig = x.clone().detach().to(device).float()\n",
    "\n",
    "    # optionally random start inside the L-inf ball\n",
    "    if random_start:\n",
    "        x_adv = x_orig + torch.empty_like(x_orig).uniform_(-eps, eps)\n",
    "        x_adv = torch.clamp(x_adv, min=clip_min, max=clip_max).detach()\n",
    "    else:\n",
    "        x_adv = x_orig.clone().detach()\n",
    "\n",
    "    y = y.to(device)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "        # ----- Liu et al. (2017): sum/average logits before loss -----\n",
    "        sum_logits = None\n",
    "        for m in models:\n",
    "            out = m(x_adv)\n",
    "            if isinstance(out, (tuple, list)):\n",
    "                out = out[0]\n",
    "            sum_logits = out if sum_logits is None else sum_logits + out\n",
    "        avg_logits = sum_logits / len(models)\n",
    "        total_loss = loss_fn(avg_logits, y)\n",
    "        # --------------------------------------------------------------\n",
    "\n",
    "        grad = torch.autograd.grad(total_loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "        # normalize (L1) like in your MI-FGSM code\n",
    "        grad = grad / (torch.norm(grad, p=1) + 1e-8)\n",
    "\n",
    "        # PGD step (no momentum) â€” use sign to match typical PGD / I-FGSM\n",
    "        step = alpha * grad.sign()\n",
    "        x_adv = x_adv.detach() + step.detach()\n",
    "\n",
    "        # project back to epsilon L-inf ball around original and clip to valid range\n",
    "        delta = torch.clamp(x_adv - x_orig, min=-eps, max=eps)\n",
    "        x_adv = torch.clamp(x_orig + delta, min=clip_min, max=clip_max).detach()\n",
    "\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc88c4e0-4dfc-496d-826c-4ae07094152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from typing import List, Union, Optional\n",
    "\n",
    "def get_gaussian_kernel(kernel_size: int = 15, sigma: float = 3.0, channels: int = 3, device='cpu'):\n",
    "    ax = torch.arange(-(kernel_size // 2), kernel_size // 2 + 1, device=device, dtype=torch.float32)\n",
    "    xx, yy = torch.meshgrid(ax, ax, indexing='xy')\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2.0 * sigma**2))\n",
    "    kernel = kernel / kernel.sum()\n",
    "    kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "    kernel = kernel.repeat(channels, 1, 1, 1)  # grouped conv\n",
    "    return kernel\n",
    "\n",
    "def diverse_input(x: torch.Tensor, prob: float = 0.7, resize_min: int = 290, resize_max: int = 330):\n",
    "    \"\"\"\n",
    "    Random resize + pad/crop. Works on x of shape (B,C,H,W).\n",
    "    If probability check fails, returns x unchanged.\n",
    "    \"\"\"\n",
    "    if prob <= 0 or torch.rand(1).item() > prob:\n",
    "        return x\n",
    "    B, C, H, W = x.shape\n",
    "    # choose a random size (square)\n",
    "    size = int(torch.randint(resize_min, resize_max + 1, (1,)).item())\n",
    "    x_resized = F.interpolate(x, size=(size, size), mode='bilinear', align_corners=False)\n",
    "    pad_h = max(0, H - size)\n",
    "    pad_w = max(0, W - size)\n",
    "    pad_top = int(torch.randint(0, pad_h + 1, (1,)).item()) if pad_h > 0 else 0\n",
    "    pad_left = int(torch.randint(0, pad_w + 1, (1,)).item()) if pad_w > 0 else 0\n",
    "    pad_bottom = pad_h - pad_top\n",
    "    pad_right = pad_w - pad_left\n",
    "    x_padded = F.pad(x_resized, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=0.0)\n",
    "    # if resized is bigger than original, center crop\n",
    "    if size > H:\n",
    "        start_h = (size - H) // 2\n",
    "        start_w = (size - W) // 2\n",
    "        x_padded = x_padded[:, :, start_h:start_h+H, start_w:start_w+W]\n",
    "    return x_padded\n",
    "\n",
    "def ensemble_ti_dim_mi_fgsm(\n",
    "    models: Union[nn.Module, List[nn.Module]],\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    eps: float = 8/255,\n",
    "    iters: int = 10,\n",
    "    alpha: Optional[float] = None,\n",
    "    decay: float = 1.0,                 # momentum mu\n",
    "    prob: float = 0.7,                  # DIM probability\n",
    "    resize_min: int = 290,\n",
    "    resize_max: int = 330,\n",
    "    kernel_size: int = 15,              # TI kernel size\n",
    "    sigma: float = 3.0,                 # TI gaussian sigma\n",
    "    ensemble_mode: str = 'logits',      # 'logits' | 'grad_avg' | 'grad_sum'\n",
    "    targeted: bool = False,\n",
    "    clip_min: float = 0.0,\n",
    "    clip_max: float = 1.0,\n",
    "    device: Optional[str] = None,\n",
    "    normalize: bool = False,\n",
    "    mean: Optional[torch.Tensor] = None,\n",
    "    std: Optional[torch.Tensor] = None,\n",
    "    loss_fn: Optional[nn.Module] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensemble TI-DIM-MI-FGSM attack.\n",
    "    - models: single model or list of models\n",
    "    - images: float tensor in [0,1], shape (B,C,H,W)\n",
    "    - labels: long tensor (B,)\n",
    "    - ensemble_mode:\n",
    "        'logits'  -> average logits then compute loss (Liu et al. style)\n",
    "        'grad_avg'-> compute per-model grads and average them\n",
    "        'grad_sum'-> sum per-model grads (sum of losses)\n",
    "    Returns adversarial images (B,C,H,W)\n",
    "    \"\"\"\n",
    "    if isinstance(models, nn.Module):\n",
    "        models = [models]\n",
    "    assert isinstance(models, (list, tuple)) and len(models) > 0\n",
    "\n",
    "    if device is None:\n",
    "        device = images.device\n",
    "\n",
    "    for m in models:\n",
    "        m.to(device).eval()\n",
    "\n",
    "    images = images.clone().detach().to(device).float()\n",
    "    orig = images.clone().detach()\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    B, C, H, W = images.shape\n",
    "    if alpha is None:\n",
    "        alpha = eps / float(iters)\n",
    "\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    kernel = get_gaussian_kernel(kernel_size, sigma, channels=C, device=device)\n",
    "    momentum = torch.zeros_like(images).to(device)\n",
    "\n",
    "    if normalize:\n",
    "        assert mean is not None and std is not None\n",
    "        mean = mean.to(device).view(1, C, 1, 1)\n",
    "        std = std.to(device).view(1, C, 1, 1)\n",
    "        def norm_fn(x): return (x - mean) / std\n",
    "    else:\n",
    "        norm_fn = lambda x: x\n",
    "\n",
    "    x_adv = images.clone().detach()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        x_adv.requires_grad_(True)\n",
    "\n",
    "        # apply diverse input transform (stochastic)\n",
    "        x_in = diverse_input(x_adv, prob=prob, resize_min=resize_min, resize_max=resize_max)\n",
    "        x_in_norm = norm_fn(x_in)\n",
    "\n",
    "        if ensemble_mode == 'logits':\n",
    "            # average logits across models then compute loss\n",
    "            sum_logits = None\n",
    "            for m in models:\n",
    "                out = m(x_in_norm)\n",
    "                if isinstance(out, (tuple, list)):\n",
    "                    out = out[0]\n",
    "                sum_logits = out if sum_logits is None else sum_logits + out\n",
    "            avg_logits = sum_logits / len(models)\n",
    "            loss = loss_fn(avg_logits, labels)\n",
    "            if targeted:\n",
    "                loss = -loss\n",
    "            grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n",
    "\n",
    "        else:\n",
    "            # compute per-model grads (w.r.t x_adv) and combine them\n",
    "            grads = []\n",
    "            for m in models:\n",
    "                out = m(x_in_norm)\n",
    "                if isinstance(out, (tuple, list)):\n",
    "                    out = out[0]\n",
    "                loss_m = loss_fn(out, labels)\n",
    "                if targeted:\n",
    "                    loss_m = -loss_m\n",
    "                g = torch.autograd.grad(loss_m, x_adv, retain_graph=True, create_graph=False)[0]\n",
    "                grads.append(g)\n",
    "            # combine grads\n",
    "            if ensemble_mode == 'grad_sum':\n",
    "                grad = sum(grads)\n",
    "            else:  # 'grad_avg'\n",
    "                grad = sum(grads) / float(len(grads))\n",
    "\n",
    "        # TI: smooth gradient with gaussian kernel (grouped conv)\n",
    "        grad_conv = F.conv2d(grad, weight=kernel, bias=None, stride=1, padding=kernel_size//2, groups=C)\n",
    "\n",
    "        # normalize gradient per-sample by mean absolute value (stability)\n",
    "        denom = torch.mean(torch.abs(grad_conv), dim=(1,2,3), keepdim=True) + 1e-12\n",
    "        grad_norm = grad_conv / denom\n",
    "\n",
    "        # momentum update\n",
    "        momentum = decay * momentum + grad_norm\n",
    "\n",
    "        # MI-FGSM step (sign of momentum)\n",
    "        if targeted:\n",
    "            x_adv = x_adv.detach() - alpha * torch.sign(momentum)\n",
    "        else:\n",
    "            x_adv = x_adv.detach() + alpha * torch.sign(momentum)\n",
    "\n",
    "        # clip to eps-ball and valid range\n",
    "        x_adv = torch.max(torch.min(x_adv, orig + eps), orig - eps)\n",
    "        x_adv = torch.clamp(x_adv, clip_min, clip_max).detach()\n",
    "\n",
    "        # zero grads\n",
    "        for m in models:\n",
    "            m.zero_grad()\n",
    "        if x_adv.grad is not None:\n",
    "            x_adv.grad.detach_()\n",
    "            x_adv.grad.zero_()\n",
    "\n",
    "    return x_adv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d8a4b-007e-4771-b8f4-43fcbe76d29c",
   "metadata": {},
   "source": [
    "### DEAA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a15e6a-d1f4-4746-8f60-a1b19c4bf681",
   "metadata": {},
   "source": [
    "#### Helper (DEAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88923601-995d-473b-8589-34c3ccae2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedModel(nn.Module):\n",
    "    def __init__(self, model, mean, std):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.register_buffer('mean', torch.tensor(mean).view(1,3,1,1))\n",
    "        self.register_buffer('std', torch.tensor(std).view(1,3,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x - self.mean) / self.std\n",
    "        return self.model(x)\n",
    "\n",
    "def get_last_linear_layer(model):\n",
    "    \"\"\"\n",
    "    Find the last Linear layer in timm models or wrapped models (NormalizedModel).\n",
    "    \"\"\"\n",
    "    # unwrap NormalizedModel if needed\n",
    "    if isinstance(model, NormalizedModel):\n",
    "        model = model.model\n",
    "\n",
    "    # Common attribute names for classifier heads\n",
    "    candidate_attrs = ['head', 'heads', 'fc', 'classifier', 'mlp_head']\n",
    "\n",
    "    for attr in candidate_attrs:\n",
    "        if hasattr(model, attr):\n",
    "            layer = getattr(model, attr)\n",
    "            # If it's a Linear layer\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                return layer\n",
    "            # If it's a Sequential or Module, search inside\n",
    "            if isinstance(layer, nn.Module):\n",
    "                last_linear = None\n",
    "                for m in reversed(list(layer.modules())):\n",
    "                    if isinstance(m, nn.Linear):\n",
    "                        last_linear = m\n",
    "                        break\n",
    "                if last_linear is not None:\n",
    "                    return last_linear\n",
    "\n",
    "    # Fallback: scan all modules\n",
    "    last_linear = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            last_linear = m\n",
    "    if last_linear is not None:\n",
    "        return last_linear\n",
    "\n",
    "    raise RuntimeError(f\"No Linear layer found in model {model.__class__.__name__}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_features_before_last_linear(model, x):\n",
    "    \"\"\"\n",
    "    Extract features before the final classifier, works for CNNs and ViTs.\n",
    "    \"\"\"\n",
    "    # unwrap NormalizedModel if present\n",
    "    if isinstance(model, NormalizedModel):\n",
    "        model = model.model\n",
    "\n",
    "    # Common classifier attributes\n",
    "    candidate_attrs = ['head', 'heads', 'fc', 'classifier', 'mlp_head']\n",
    "    classifier = None\n",
    "    for attr in candidate_attrs:\n",
    "        if hasattr(model, attr):\n",
    "            classifier = getattr(model, attr)\n",
    "            break\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        features['feat'] = input[0].detach()\n",
    "\n",
    "    if classifier is not None:\n",
    "        handle = classifier.register_forward_hook(hook)\n",
    "    else:\n",
    "        # fallback: attach hook to last module\n",
    "        last_module = list(model.modules())[-1]\n",
    "        handle = last_module.register_forward_hook(hook)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    handle.remove()\n",
    "\n",
    "    if 'feat' not in features:\n",
    "        raise RuntimeError(f\"Failed to capture features from model {model.__class__.__name__}\")\n",
    "\n",
    "    return features['feat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa42722-2481-4568-be45-d95f0108ee8f",
   "metadata": {},
   "source": [
    "#### DEAA Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31529987-19ce-44ae-b8ff-1c9ffa9da1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softmax(x, dim=0):\n",
    "    return F.softmax(x, dim=dim)\n",
    "\n",
    "\n",
    "def cosine_similarity(x, y, dim=1, eps=1e-8):\n",
    "    return F.cosine_similarity(x, y, dim=dim, eps=eps)\n",
    "\n",
    "\n",
    "# ðŸ‘€ Dummy visualization (replace with your function)\n",
    "def visualize_test_and_roc(test_img, roc_imgs, local_labels):\n",
    "    print(\"Visualization placeholder: Test image + RoC samples\")\n",
    "    print(f\"RoC labels: {local_labels}\")\n",
    "\n",
    "\n",
    "class VisionDES_2: \n",
    "    def __init__(self, dsel_dataset, pool): \n",
    "        self.dsel_dataset = dsel_dataset\n",
    "        self.dsel_loader = DataLoader(dsel_dataset, batch_size=32, shuffle=False) \n",
    "        self.dino_model = timm.create_model('vit_base_patch16_224.dino', pretrained=True).to(device)\n",
    "        self.dino_model.eval()  \n",
    "        self.pool = pool \n",
    "\n",
    "        self.suspected_model_votes = [] \n",
    "        \n",
    "        \n",
    "    def dino_embedder(self, images):\n",
    "        if images.shape[1] == 1:\n",
    "            images = images.repeat(1, 3, 1, 1)\n",
    "        return self.dino_model.forward_features(images)\n",
    "\n",
    "\n",
    "    def fit(self): \n",
    "        dsel_embeddings = []\n",
    "        dsel_labels = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.dsel_loader):\n",
    "                imgs = imgs.to(device)\n",
    "                embs = self.dino_embedder(imgs).cpu()  \n",
    "                dsel_embeddings.append(embs)\n",
    "                dsel_labels.append(labels)\n",
    "    \n",
    "        # Keep as tensor\n",
    "        dsel_embeddings_tensor = torch.cat(dsel_embeddings).detach().cpu()  \n",
    "        cls_tensor = dsel_embeddings_tensor[:, 0, :]  \n",
    "    \n",
    "        # Convert to NumPy\n",
    "        cls_embeddings = np.ascontiguousarray(cls_tensor.numpy(), dtype='float32')\n",
    "        self.dsel_embeddings = cls_embeddings\n",
    "        self.dsel_labels = torch.cat(dsel_labels).numpy()\n",
    "    \n",
    "        # Build FAISS index\n",
    "        embedding_dim = cls_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
    "        self.index.add(cls_embeddings)\n",
    "\n",
    "    \n",
    "    def get_top_n_competent_models(self, test_img, k=7, top_n=3, use_sim=False, sim_threshold=0, alpha=0.6):\n",
    "        # Step 1: Get DINO CLS embedding for the test image\n",
    "        img_for_dino = test_img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_emb = self.dino_model.forward_features(img_for_dino).cpu().numpy().astype('float32')\n",
    "            test_emb = test_emb[:, 0, :]  # CLS token only\n",
    "    \n",
    "        # Step 2: Find k nearest neighbors in FAISS\n",
    "        distances, neighbors = self.index.search(test_emb, k)\n",
    "        neighbor_idxs = neighbors[0]\n",
    "        local_labels = np.array(self.dsel_labels[neighbor_idxs]).flatten()\n",
    "    \n",
    "        # Step 3: Get RoC images\n",
    "        with torch.no_grad():\n",
    "            roc_imgs = torch.stack([self.dsel_dataset[idx][0] for idx in neighbor_idxs]).to(device)\n",
    "    \n",
    "        # Step 4: Evaluate classifiers\n",
    "        competences, feature_similarities, correct_counts, grad_vectors  = [], [], [], []\n",
    "    \n",
    "        for clf in self.pool:\n",
    "            clf.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = clf(roc_imgs)\n",
    "                preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                correct = (preds == local_labels).sum()\n",
    "                competences.append(correct / k)\n",
    "                correct_counts.append(correct)\n",
    "    \n",
    "                # Feature similarity\n",
    "                test_feat = get_features_before_last_linear(clf, test_img.unsqueeze(0).to(device))\n",
    "                roc_feats = get_features_before_last_linear(clf, roc_imgs)\n",
    "                mean_feat = roc_feats.mean(dim=0, keepdim=True)\n",
    "                sim = cosine_similarity(test_feat.flatten().unsqueeze(0), mean_feat.flatten().unsqueeze(0))\n",
    "                feature_similarities.append(sim.item())\n",
    "\n",
    "            test_img_req = test_img.clone().detach().unsqueeze(0).to(device)\n",
    "            test_img_req.requires_grad_(True)\n",
    "\n",
    "            out = clf(test_img_req)\n",
    "            pseudo_label = out.argmax(dim=1)  # keep grad path alive\n",
    "            loss = F.cross_entropy(out, pseudo_label)\n",
    "            grad = torch.autograd.grad(loss, test_img_req)[0]\n",
    "            grad_vec = grad.flatten()  # flatten for cosine similarity\n",
    "            grad_vectors.append(grad_vec.cpu())\n",
    "\n",
    "\n",
    "        # Step 5: Compute gradient-based diversity\n",
    "        grads_tensor = torch.stack(grad_vectors).to(device) \n",
    "        # L2-normalize per-model gradient vector\n",
    "        grads_norm = F.normalize(grads_tensor, p=2, dim=1, eps=1e-8)  # (K, D)\n",
    "        \n",
    "        # compute cosine similarity matrix\n",
    "        cos_sim = grads_norm @ grads_norm.t()  # (K, K)  (equivalent to pairwise cosine) \n",
    "\n",
    "        # zero out diagonal (self-sim = 1)\n",
    "        K = cos_sim.size(0)\n",
    "        cos_sim.fill_diagonal_(0.0)\n",
    "        \n",
    "        # average similarity over other models only\n",
    "        mean_sim_other = cos_sim.sum(dim=1) / float(K - 1)  # (K,)\n",
    "        \n",
    "        # diversity score: lower similarity -> higher diversity\n",
    "        diversity_scores = (mean_sim_other).cpu().numpy()\n",
    "        diversity_scores = (diversity_scores - diversity_scores.min()) / (diversity_scores.max() - diversity_scores.min() + 1e-8)\n",
    "\n",
    "        # print(\"Diversity\", diversity_scores)\n",
    "        # print(\"Competences\", competences) \n",
    "\n",
    "        final_scores = [alpha * c + (1 - alpha) * d for c, d in zip(competences, diversity_scores)] \n",
    "    \n",
    "        # Step 7: Select top_n models\n",
    "        top_indices = np.argsort(final_scores)[::-1][:top_n]\n",
    "        # print(\"final_scores\", final_scores)\n",
    "        # print(\"Top_indices\", top_indices)\n",
    "        top_models = [self.pool[i] for i in top_indices]\n",
    "    \n",
    "        return top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31f93c-5cbf-4a1c-94f0-4b3c77b4e6fa",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05ce9e51-afa3-4a3b-b7da-96c5acb05706",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_models = [\n",
    "    get_models(\"imagenet\", \"resnet18\", \"resnet18\"), \n",
    "    get_models(\"imagenet\", \"inception_v3\", \"inc_v3\"), \n",
    "    get_models(\"imagenet\", \"deit_tiny_patch16_224\", \"deit_t\"),\n",
    "    get_models(\"imagenet\", \"vit_tiny_patch16_224\", \"vit_t\"), \n",
    "    get_models(\"imagenet\", \"efficientnet_b0\", \"efficientnet_b0\"), \n",
    "    get_models(\"imagenet\", \"xcit_tiny_12_p8_224\", \"swin_t\"), \n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03051e-2db8-4be4-914a-6f3c642eaabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [03:50<00:00,  4.07it/s]\n"
     ]
    }
   ],
   "source": [
    "des_model = VisionDES_2(val_dataset, ens_models)\n",
    "des_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44047ef-4d82-4031-9d15-108d93034d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_index = des_model.index\n",
    "saved_dsel_embeddings = des_model.dsel_embeddings \n",
    "saved_dsel_labels = des_model.dsel_labels\n",
    "\n",
    "# des_model.index = saved_index \n",
    "# des_model.dsel_embeddings = saved_dsel_embeddings \n",
    "# des_model.dsel_labels = saved_dsel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d90ec98b-90e2-4008-982a-53ff0f51cce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating MI-FGSM adversarials (GPU): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [27:10<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 1000 adversarial images. Shape: torch.Size([1000, 3, 224, 224])\n",
      "Noise (1 - SSIM): mean=0.161277, std=0.071326, min=0.014407, max=0.538927\n",
      "Mean absolute pixel diff (after clamp to [0,1]): mean=0.025323, std=0.002007\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional.image import structural_similarity_index_measure as ssim\n",
    "import torch\n",
    "\n",
    "# --- before loop (clear previous lists) ---\n",
    "adv_list = []\n",
    "orig_list = []\n",
    "labels_list = []\n",
    "noise_rates = []\n",
    "pixel_diffs = []\n",
    "\n",
    "eps = 8/255\n",
    "alpha = 2/255\n",
    "iters = 10\n",
    "\n",
    "def ensure_batch(x):\n",
    "    return x if x.dim() == 4 else x.unsqueeze(0)\n",
    "\n",
    "def to_unit_range(x):\n",
    "    \"\"\"\n",
    "    Ensure x is in [0,1]. If tensor values appear to be in [0,255] (max>1.5),\n",
    "    convert by dividing by 255. Returns a float tensor on same device.\n",
    "    \"\"\"\n",
    "    x = ensure_batch(x).float()\n",
    "    if x.max().item() > 1.5:\n",
    "        x = x / 255.0\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "# --- attack loop (same as yours, but using the simplified functions) ---\n",
    "for img, label in tqdm(hard_loader, desc=\"Generating MI-FGSM adversarials (GPU)\"):\n",
    "    img, label = img.to(device), label.to(device)\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        selected_models = des_model.get_top_n_competent_models(\n",
    "                img[0],                    \n",
    "                k=7,\n",
    "                top_n=2,\n",
    "                use_sim=False,\n",
    "                sim_threshold=0,\n",
    "                alpha=0.4\n",
    "            )\n",
    "        \n",
    "        adv_img = ensemble_mi_fgsm(selected_models, img, label, eps=eps, alpha=alpha, iters=iters, clip_min=0.0, clip_max=1.0, device=device)\n",
    "        # adv_img = ensemble_pgd(selected_models, img, label, eps=eps, alpha=alpha, iters=iters, clip_min=0.0, clip_max=1.0, device=device)\n",
    "        # adv_img = ensemble_ti_dim_mi_fgsm(ens_models, img, label,\n",
    "        #                       eps=8/255, iters=10, alpha=2/255,\n",
    "        #                       decay=1.0, prob=0.7,\n",
    "        #                       kernel_size=15, sigma=1.0,\n",
    "        #                       ensemble_mode='logits',\n",
    "        #                       device=device)\n",
    "    # store for later (move to CPU)\n",
    "    adv_list.append(adv_img.squeeze(0).cpu())\n",
    "    orig_list.append(img.squeeze(0).cpu())\n",
    "    labels_list.append(label.squeeze(0).cpu())\n",
    "\n",
    "    # compute SSIM and pixel diffs on [0,1] images\n",
    "    img_for_ssim = to_unit_range(img)       # (1,C,H,W) in [0,1]\n",
    "    adv_for_ssim = to_unit_range(adv_img)   # (1,C,H,W) in [0,1]\n",
    "\n",
    "    ssim_val = ssim(adv_for_ssim, img_for_ssim)  # scalar tensor\n",
    "    noise_rates.append((1.0 - float(ssim_val)))\n",
    "    pixel_diffs.append((adv_for_ssim - img_for_ssim).abs().mean().item())\n",
    "\n",
    "# --- stack everything on CPU ---\n",
    "adv_all = torch.stack(adv_list).cpu()\n",
    "orig_all = torch.stack(orig_list).cpu()\n",
    "labels_all = torch.stack(labels_list).cpu()\n",
    "\n",
    "noise_rates = torch.tensor(noise_rates)\n",
    "pixel_diffs = torch.tensor(pixel_diffs)\n",
    "\n",
    "print(f\"âœ… Generated {adv_all.size(0)} adversarial images. Shape: {adv_all.shape}\")\n",
    "print(f\"Noise (1 - SSIM): mean={noise_rates.mean():.6f}, std={noise_rates.std():.6f}, min={noise_rates.min():.6f}, max={noise_rates.max():.6f}\")\n",
    "print(f\"Mean absolute pixel diff (after clamp to [0,1]): mean={pixel_diffs.mean():.6f}, std={pixel_diffs.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266581d-8177-4b69-919b-0c67a35e91bb",
   "metadata": {},
   "source": [
    "### Test on Target Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23929e0d-2e9d-4559-9351-d6aa39f0c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32  # tune this for your GPU\n",
    "dataset = TensorDataset(adv_all, labels_all)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da230ba-1952-4a33-a373-37ec28a165b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_models = [\n",
    "    # get_models(\"imagenet\", \"resnet152\", \"resnet152\"),\n",
    "    # get_models(\"imagenet\", \"wide_resnet101_2\", \"wrn101_2\"),     \n",
    "    # get_models(\"imagenet\", \"regnety_320\", \"regnety_320\"),\n",
    "    # get_models(\"imagenet\", \"vgg19\", \"vgg19\"),\n",
    "    # get_models(\"imagenet\", \"vit_base_patch16_224\", \"vit_b\"),\n",
    "    # get_models(\"imagenet\", \"deit_base_patch16_224\", \"deit_b\"),\n",
    "    get_models(\"imagenet\", \"swin_base_patch4_window7_224\", \"swin_b\"), \n",
    "    get_models(\"imagenet\", \"mixer_b16_224\", \"vit_t\"), \n",
    "    get_models(\"imagenet\", \"convmixer_768_32\", \"vit_t\")\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3f48182-e1fa-4411-a6a9-10fc0c5a67b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 66.80%  (668/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:03<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 85.60%  (856/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ASR Sequential: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: ASR = 91.50%  (915/1000 fooled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for t_model in target_models:\n",
    "        name = getattr(t_model, \"name\", t_model.__class__.__name__)\n",
    "        t_model.eval()\n",
    "        t_model.to(device)\n",
    "\n",
    "        fooled = 0\n",
    "        total = 0\n",
    "\n",
    "        for imgs_cpu, labels_cpu in tqdm(loader, desc=f\"ASR {name}\"):\n",
    "            # Move to device here\n",
    "            imgs = imgs_cpu.to(device, non_blocking=True)\n",
    "            labels = labels_cpu.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = t_model(imgs)\n",
    "            if isinstance(outputs, (tuple, list)):\n",
    "                outputs = outputs[0]\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            fooled += (preds != labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # free cache per batch (helps on tight GPUs)\n",
    "            if device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        asr = 100.0 * fooled / total if total > 0 else 0.0\n",
    "        print(f\"{name}: ASR = {asr:.2f}%  ({fooled}/{total} fooled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74475f76-a159-4b41-b56b-79c8f488e4ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd273ca3-576d-4e9f-9040-387f4f7e2b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
